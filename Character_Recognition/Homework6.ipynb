{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "import scipy as sc\n",
    "from scipy.stats import norm\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'/Users/westleykirkham/PycharmProjects/Translator/' # use your path\n",
    "\n",
    "def makeDataFrame(path, filename):\n",
    "    allFiles = glob.glob(path + filename)\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "    frame = pd.concat(list_)\n",
    "    # Delete columns we won't be using\n",
    "    nf = frame.drop('font', axis=1)\n",
    "    nf = nf.drop('fontVariant', axis=1)\n",
    "    nf = nf.drop('strength', axis=1)\n",
    "    nf = nf.drop('italic', axis=1)\n",
    "    nf = nf.drop('orientation', axis=1)\n",
    "    nf = nf.drop('m_top', axis=1)\n",
    "    nf = nf.drop('m_left', axis=1)\n",
    "    nf = nf.drop('originalH', axis=1)\n",
    "    nf = nf.drop('originalW', axis=1)\n",
    "    nf = nf.drop('h', axis=1)\n",
    "    nf = nf.drop('w', axis=1)\n",
    "    return nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = makeDataFrame(path, \"KUNSTLER.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 2 numpy arrays: Xs which is a #samples x 20 x 20 array containing the pixel values, \n",
    "#and Ys which is a #samples x 1 array containing the ascii vales for each character. \n",
    "#Divides everything in the Xs array by 255 to scale the values from 0-1.\n",
    "def dfTransform(df):\n",
    "    Ys = df.iloc[:,:1].values\n",
    "    Xs = df.iloc[:,1:].values / 255\n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s, Y_s = dfTransform(nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61442],\n",
       "       [61441],\n",
       "       [ 9674],\n",
       "       [ 8805],\n",
       "       [ 8804],\n",
       "       [ 8800],\n",
       "       [ 8776],\n",
       "       [ 8747],\n",
       "       [ 8734],\n",
       "       [ 8730],\n",
       "       [ 8729],\n",
       "       [ 8725],\n",
       "       [ 8722],\n",
       "       [ 8721],\n",
       "       [ 8719],\n",
       "       [ 8710],\n",
       "       [ 8706],\n",
       "       [ 8486],\n",
       "       [ 8482],\n",
       "       [ 8364],\n",
       "       [ 8250],\n",
       "       [ 8249],\n",
       "       [ 8240],\n",
       "       [ 8230],\n",
       "       [ 8226],\n",
       "       [ 8225],\n",
       "       [ 8224],\n",
       "       [ 8222],\n",
       "       [ 8221],\n",
       "       [ 8220],\n",
       "       [ 8218],\n",
       "       [ 8217],\n",
       "       [ 8216],\n",
       "       [ 8212],\n",
       "       [ 8211],\n",
       "       [  960],\n",
       "       [  733],\n",
       "       [  732],\n",
       "       [  731],\n",
       "       [  730],\n",
       "       [  729],\n",
       "       [  728],\n",
       "       [  713],\n",
       "       [  711],\n",
       "       [  710],\n",
       "       [  402],\n",
       "       [  382],\n",
       "       [  381],\n",
       "       [  376],\n",
       "       [  353],\n",
       "       [  352],\n",
       "       [  339],\n",
       "       [  338],\n",
       "       [  322],\n",
       "       [  321],\n",
       "       [  305],\n",
       "       [  254],\n",
       "       [  253],\n",
       "       [  252],\n",
       "       [  251],\n",
       "       [  250],\n",
       "       [  249],\n",
       "       [  248],\n",
       "       [  247],\n",
       "       [  246],\n",
       "       [  245],\n",
       "       [  244],\n",
       "       [  243],\n",
       "       [  242],\n",
       "       [  241],\n",
       "       [  240],\n",
       "       [  239],\n",
       "       [  238],\n",
       "       [  237],\n",
       "       [  236],\n",
       "       [  235],\n",
       "       [  234],\n",
       "       [  233],\n",
       "       [  232],\n",
       "       [  231],\n",
       "       [  230],\n",
       "       [  229],\n",
       "       [  228],\n",
       "       [  227],\n",
       "       [  226],\n",
       "       [  225],\n",
       "       [  224],\n",
       "       [  223],\n",
       "       [  222],\n",
       "       [  221],\n",
       "       [  220],\n",
       "       [  219],\n",
       "       [  218],\n",
       "       [  217],\n",
       "       [  216],\n",
       "       [  215],\n",
       "       [  214],\n",
       "       [  213],\n",
       "       [  212],\n",
       "       [  211],\n",
       "       [  210],\n",
       "       [  209],\n",
       "       [  208],\n",
       "       [  207],\n",
       "       [  206],\n",
       "       [  205],\n",
       "       [  204],\n",
       "       [  203],\n",
       "       [  202],\n",
       "       [  201],\n",
       "       [  200],\n",
       "       [  199],\n",
       "       [  198],\n",
       "       [  197],\n",
       "       [  196],\n",
       "       [  195],\n",
       "       [  194],\n",
       "       [  193],\n",
       "       [  192],\n",
       "       [  191],\n",
       "       [  190],\n",
       "       [  189],\n",
       "       [  188],\n",
       "       [  187],\n",
       "       [  186],\n",
       "       [  185],\n",
       "       [  184],\n",
       "       [  183],\n",
       "       [  182],\n",
       "       [  181],\n",
       "       [  180],\n",
       "       [  179],\n",
       "       [  178],\n",
       "       [  177],\n",
       "       [  176],\n",
       "       [  175],\n",
       "       [  174],\n",
       "       [  173],\n",
       "       [  172],\n",
       "       [  171],\n",
       "       [  170],\n",
       "       [  169],\n",
       "       [  168],\n",
       "       [  167],\n",
       "       [  166],\n",
       "       [  165],\n",
       "       [  164],\n",
       "       [  163],\n",
       "       [  162],\n",
       "       [  161],\n",
       "       [  126],\n",
       "       [  125],\n",
       "       [  124],\n",
       "       [  123],\n",
       "       [  122],\n",
       "       [  121],\n",
       "       [  120],\n",
       "       [  119],\n",
       "       [  118],\n",
       "       [  117],\n",
       "       [  116],\n",
       "       [  115],\n",
       "       [  114],\n",
       "       [  113],\n",
       "       [  112],\n",
       "       [  111],\n",
       "       [  110],\n",
       "       [  109],\n",
       "       [  108],\n",
       "       [  107],\n",
       "       [  106],\n",
       "       [  105],\n",
       "       [  104],\n",
       "       [  103],\n",
       "       [  102],\n",
       "       [  101],\n",
       "       [  100],\n",
       "       [   99],\n",
       "       [   98],\n",
       "       [   97],\n",
       "       [   96],\n",
       "       [   95],\n",
       "       [   94],\n",
       "       [   93],\n",
       "       [   92],\n",
       "       [   91],\n",
       "       [   90],\n",
       "       [   89],\n",
       "       [   88],\n",
       "       [   87],\n",
       "       [   86],\n",
       "       [   85],\n",
       "       [   84],\n",
       "       [   83],\n",
       "       [   82],\n",
       "       [   81],\n",
       "       [   80],\n",
       "       [   79],\n",
       "       [   78],\n",
       "       [   77],\n",
       "       [   76],\n",
       "       [   75],\n",
       "       [   74],\n",
       "       [   73],\n",
       "       [   72],\n",
       "       [   71],\n",
       "       [   70],\n",
       "       [   69],\n",
       "       [   68],\n",
       "       [   67],\n",
       "       [   66],\n",
       "       [   65],\n",
       "       [   64],\n",
       "       [   63],\n",
       "       [   62],\n",
       "       [   61],\n",
       "       [   60],\n",
       "       [   59],\n",
       "       [   58],\n",
       "       [   57],\n",
       "       [   56],\n",
       "       [   55],\n",
       "       [   54],\n",
       "       [   53],\n",
       "       [   52],\n",
       "       [   51],\n",
       "       [   50],\n",
       "       [   49],\n",
       "       [   48],\n",
       "       [   47],\n",
       "       [   46],\n",
       "       [   45],\n",
       "       [   44],\n",
       "       [   43],\n",
       "       [   42],\n",
       "       [   41],\n",
       "       [   40],\n",
       "       [   39],\n",
       "       [   38],\n",
       "       [   37],\n",
       "       [   36],\n",
       "       [   35],\n",
       "       [   34],\n",
       "       [   33],\n",
       "       [61442],\n",
       "       [61441],\n",
       "       [ 9674],\n",
       "       [ 8805],\n",
       "       [ 8804],\n",
       "       [ 8800],\n",
       "       [ 8776],\n",
       "       [ 8747],\n",
       "       [ 8734],\n",
       "       [ 8730],\n",
       "       [ 8729],\n",
       "       [ 8725],\n",
       "       [ 8722],\n",
       "       [ 8721],\n",
       "       [ 8719],\n",
       "       [ 8710],\n",
       "       [ 8706],\n",
       "       [ 8486],\n",
       "       [ 8482],\n",
       "       [ 8364],\n",
       "       [ 8250],\n",
       "       [ 8249],\n",
       "       [ 8240],\n",
       "       [ 8230],\n",
       "       [ 8226],\n",
       "       [ 8225],\n",
       "       [ 8224],\n",
       "       [ 8222],\n",
       "       [ 8221],\n",
       "       [ 8220],\n",
       "       [ 8218],\n",
       "       [ 8217],\n",
       "       [ 8216],\n",
       "       [ 8212],\n",
       "       [ 8211],\n",
       "       [  960],\n",
       "       [  733],\n",
       "       [  732],\n",
       "       [  731],\n",
       "       [  730],\n",
       "       [  729],\n",
       "       [  728],\n",
       "       [  713],\n",
       "       [  711],\n",
       "       [  710],\n",
       "       [  402],\n",
       "       [  382],\n",
       "       [  381],\n",
       "       [  376],\n",
       "       [  353],\n",
       "       [  352],\n",
       "       [  339],\n",
       "       [  338],\n",
       "       [  322],\n",
       "       [  321],\n",
       "       [  305],\n",
       "       [  254],\n",
       "       [  253],\n",
       "       [  252],\n",
       "       [  251],\n",
       "       [  250],\n",
       "       [  249],\n",
       "       [  248],\n",
       "       [  247],\n",
       "       [  246],\n",
       "       [  245],\n",
       "       [  244],\n",
       "       [  243],\n",
       "       [  242],\n",
       "       [  241],\n",
       "       [  240],\n",
       "       [  239],\n",
       "       [  238],\n",
       "       [  237],\n",
       "       [  236],\n",
       "       [  235],\n",
       "       [  234],\n",
       "       [  233],\n",
       "       [  232],\n",
       "       [  231],\n",
       "       [  230],\n",
       "       [  229],\n",
       "       [  228],\n",
       "       [  227],\n",
       "       [  226],\n",
       "       [  225],\n",
       "       [  224],\n",
       "       [  223],\n",
       "       [  222],\n",
       "       [  221],\n",
       "       [  220],\n",
       "       [  219],\n",
       "       [  218],\n",
       "       [  217],\n",
       "       [  216],\n",
       "       [  215],\n",
       "       [  214],\n",
       "       [  213],\n",
       "       [  212],\n",
       "       [  211],\n",
       "       [  210],\n",
       "       [  209],\n",
       "       [  208],\n",
       "       [  207],\n",
       "       [  206],\n",
       "       [  205],\n",
       "       [  204],\n",
       "       [  203],\n",
       "       [  202],\n",
       "       [  201],\n",
       "       [  200],\n",
       "       [  199],\n",
       "       [  198],\n",
       "       [  197],\n",
       "       [  196],\n",
       "       [  195],\n",
       "       [  194],\n",
       "       [  193],\n",
       "       [  192],\n",
       "       [  191],\n",
       "       [  190],\n",
       "       [  189],\n",
       "       [  188],\n",
       "       [  187],\n",
       "       [  186],\n",
       "       [  185],\n",
       "       [  184],\n",
       "       [  183],\n",
       "       [  182],\n",
       "       [  181],\n",
       "       [  180],\n",
       "       [  179],\n",
       "       [  178],\n",
       "       [  177],\n",
       "       [  176],\n",
       "       [  175],\n",
       "       [  174],\n",
       "       [  173],\n",
       "       [  172],\n",
       "       [  171],\n",
       "       [  170],\n",
       "       [  169],\n",
       "       [  168],\n",
       "       [  167],\n",
       "       [  166],\n",
       "       [  165],\n",
       "       [  164],\n",
       "       [  163],\n",
       "       [  162],\n",
       "       [  161],\n",
       "       [  126],\n",
       "       [  125],\n",
       "       [  124],\n",
       "       [  123],\n",
       "       [  122],\n",
       "       [  121],\n",
       "       [  120],\n",
       "       [  119],\n",
       "       [  118],\n",
       "       [  117],\n",
       "       [  116],\n",
       "       [  115],\n",
       "       [  114],\n",
       "       [  113],\n",
       "       [  112],\n",
       "       [  111],\n",
       "       [  110],\n",
       "       [  109],\n",
       "       [  108],\n",
       "       [  107],\n",
       "       [  106],\n",
       "       [  105],\n",
       "       [  104],\n",
       "       [  103],\n",
       "       [  102],\n",
       "       [  101],\n",
       "       [  100],\n",
       "       [   99],\n",
       "       [   98],\n",
       "       [   97],\n",
       "       [   96],\n",
       "       [   95],\n",
       "       [   94],\n",
       "       [   93],\n",
       "       [   92],\n",
       "       [   91],\n",
       "       [   90],\n",
       "       [   89],\n",
       "       [   88],\n",
       "       [   87],\n",
       "       [   86],\n",
       "       [   85],\n",
       "       [   84],\n",
       "       [   83],\n",
       "       [   82],\n",
       "       [   81],\n",
       "       [   80],\n",
       "       [   79],\n",
       "       [   78],\n",
       "       [   77],\n",
       "       [   76],\n",
       "       [   75],\n",
       "       [   74],\n",
       "       [   73],\n",
       "       [   72],\n",
       "       [   71],\n",
       "       [   70],\n",
       "       [   69],\n",
       "       [   68],\n",
       "       [   67],\n",
       "       [   66],\n",
       "       [   65],\n",
       "       [   64],\n",
       "       [   63],\n",
       "       [   62],\n",
       "       [   61],\n",
       "       [   60],\n",
       "       [   59],\n",
       "       [   58],\n",
       "       [   57],\n",
       "       [   56],\n",
       "       [   55],\n",
       "       [   54],\n",
       "       [   53],\n",
       "       [   52],\n",
       "       [   51],\n",
       "       [   50],\n",
       "       [   49],\n",
       "       [   48],\n",
       "       [   47],\n",
       "       [   46],\n",
       "       [   45],\n",
       "       [   44],\n",
       "       [   43],\n",
       "       [   42],\n",
       "       [   41],\n",
       "       [   40],\n",
       "       [   39],\n",
       "       [   38],\n",
       "       [   37],\n",
       "       [   36],\n",
       "       [   35],\n",
       "       [   34],\n",
       "       [   33],\n",
       "       [61442],\n",
       "       [61441],\n",
       "       [ 9674],\n",
       "       [ 8805],\n",
       "       [ 8804],\n",
       "       [ 8800],\n",
       "       [ 8776],\n",
       "       [ 8747],\n",
       "       [ 8734],\n",
       "       [ 8730],\n",
       "       [ 8729],\n",
       "       [ 8725],\n",
       "       [ 8722],\n",
       "       [ 8721],\n",
       "       [ 8719],\n",
       "       [ 8710],\n",
       "       [ 8706],\n",
       "       [ 8486],\n",
       "       [ 8482],\n",
       "       [ 8364],\n",
       "       [ 8250],\n",
       "       [ 8249],\n",
       "       [ 8240],\n",
       "       [ 8230],\n",
       "       [ 8226],\n",
       "       [ 8225],\n",
       "       [ 8224],\n",
       "       [ 8222],\n",
       "       [ 8221],\n",
       "       [ 8220],\n",
       "       [ 8218],\n",
       "       [ 8217],\n",
       "       [ 8216],\n",
       "       [ 8212],\n",
       "       [ 8211],\n",
       "       [  960],\n",
       "       [  733],\n",
       "       [  732],\n",
       "       [  731],\n",
       "       [  730],\n",
       "       [  729],\n",
       "       [  728],\n",
       "       [  713],\n",
       "       [  711],\n",
       "       [  710],\n",
       "       [  402],\n",
       "       [  382],\n",
       "       [  381],\n",
       "       [  376],\n",
       "       [  353],\n",
       "       [  352],\n",
       "       [  339],\n",
       "       [  338],\n",
       "       [  322],\n",
       "       [  321],\n",
       "       [  305],\n",
       "       [  254],\n",
       "       [  253],\n",
       "       [  252],\n",
       "       [  251],\n",
       "       [  250],\n",
       "       [  249],\n",
       "       [  248],\n",
       "       [  247],\n",
       "       [  246],\n",
       "       [  245],\n",
       "       [  244],\n",
       "       [  243],\n",
       "       [  242],\n",
       "       [  241],\n",
       "       [  240],\n",
       "       [  239],\n",
       "       [  238],\n",
       "       [  237],\n",
       "       [  236],\n",
       "       [  235],\n",
       "       [  234],\n",
       "       [  233],\n",
       "       [  232],\n",
       "       [  231],\n",
       "       [  230],\n",
       "       [  229],\n",
       "       [  228],\n",
       "       [  227],\n",
       "       [  226],\n",
       "       [  225],\n",
       "       [  224],\n",
       "       [  223],\n",
       "       [  222],\n",
       "       [  221],\n",
       "       [  220],\n",
       "       [  219],\n",
       "       [  218],\n",
       "       [  217],\n",
       "       [  216],\n",
       "       [  215],\n",
       "       [  214],\n",
       "       [  213],\n",
       "       [  212],\n",
       "       [  211],\n",
       "       [  210],\n",
       "       [  209],\n",
       "       [  208],\n",
       "       [  207],\n",
       "       [  206],\n",
       "       [  205],\n",
       "       [  204],\n",
       "       [  203],\n",
       "       [  202],\n",
       "       [  201],\n",
       "       [  200],\n",
       "       [  199],\n",
       "       [  198],\n",
       "       [  197],\n",
       "       [  196],\n",
       "       [  195],\n",
       "       [  194],\n",
       "       [  193],\n",
       "       [  192],\n",
       "       [  191],\n",
       "       [  190],\n",
       "       [  189],\n",
       "       [  188],\n",
       "       [  187],\n",
       "       [  186],\n",
       "       [  185],\n",
       "       [  184],\n",
       "       [  183],\n",
       "       [  182],\n",
       "       [  181],\n",
       "       [  180],\n",
       "       [  179],\n",
       "       [  178],\n",
       "       [  177],\n",
       "       [  176],\n",
       "       [  175],\n",
       "       [  174],\n",
       "       [  173],\n",
       "       [  172],\n",
       "       [  171],\n",
       "       [  170],\n",
       "       [  169],\n",
       "       [  168],\n",
       "       [  167],\n",
       "       [  166],\n",
       "       [  165],\n",
       "       [  164],\n",
       "       [  163],\n",
       "       [  162],\n",
       "       [  161],\n",
       "       [  126],\n",
       "       [  125],\n",
       "       [  124],\n",
       "       [  123],\n",
       "       [  122],\n",
       "       [  121],\n",
       "       [  120],\n",
       "       [  119],\n",
       "       [  118],\n",
       "       [  117],\n",
       "       [  116],\n",
       "       [  115],\n",
       "       [  114],\n",
       "       [  113],\n",
       "       [  112],\n",
       "       [  111],\n",
       "       [  110],\n",
       "       [  109],\n",
       "       [  108],\n",
       "       [  107],\n",
       "       [  106],\n",
       "       [  105],\n",
       "       [  104],\n",
       "       [  103],\n",
       "       [  102],\n",
       "       [  101],\n",
       "       [  100],\n",
       "       [   99],\n",
       "       [   98],\n",
       "       [   97],\n",
       "       [   96],\n",
       "       [   95],\n",
       "       [   94],\n",
       "       [   93],\n",
       "       [   92],\n",
       "       [   91],\n",
       "       [   90],\n",
       "       [   89],\n",
       "       [   88],\n",
       "       [   87],\n",
       "       [   86],\n",
       "       [   85],\n",
       "       [   84],\n",
       "       [   83],\n",
       "       [   82],\n",
       "       [   81],\n",
       "       [   80],\n",
       "       [   79],\n",
       "       [   78],\n",
       "       [   77],\n",
       "       [   76],\n",
       "       [   75],\n",
       "       [   74],\n",
       "       [   73],\n",
       "       [   72],\n",
       "       [   71],\n",
       "       [   70],\n",
       "       [   69],\n",
       "       [   68],\n",
       "       [   67],\n",
       "       [   66],\n",
       "       [   65],\n",
       "       [   64],\n",
       "       [   63],\n",
       "       [   62],\n",
       "       [   61],\n",
       "       [   60],\n",
       "       [   59],\n",
       "       [   58],\n",
       "       [   57],\n",
       "       [   56],\n",
       "       [   55],\n",
       "       [   54],\n",
       "       [   53],\n",
       "       [   52],\n",
       "       [   51],\n",
       "       [   50],\n",
       "       [   49],\n",
       "       [   48],\n",
       "       [   47],\n",
       "       [   46],\n",
       "       [   45],\n",
       "       [   44],\n",
       "       [   43],\n",
       "       [   42],\n",
       "       [   41],\n",
       "       [   40],\n",
       "       [   39],\n",
       "       [   38],\n",
       "       [   37],\n",
       "       [   36],\n",
       "       [   35],\n",
       "       [   34],\n",
       "       [   33],\n",
       "       [61442],\n",
       "       [61441],\n",
       "       [ 9674],\n",
       "       [ 8805],\n",
       "       [ 8804],\n",
       "       [ 8800],\n",
       "       [ 8776],\n",
       "       [ 8747],\n",
       "       [ 8734],\n",
       "       [ 8730],\n",
       "       [ 8729],\n",
       "       [ 8725],\n",
       "       [ 8722],\n",
       "       [ 8721],\n",
       "       [ 8719],\n",
       "       [ 8710],\n",
       "       [ 8706],\n",
       "       [ 8486],\n",
       "       [ 8482],\n",
       "       [ 8364],\n",
       "       [ 8250],\n",
       "       [ 8249],\n",
       "       [ 8240],\n",
       "       [ 8230],\n",
       "       [ 8226],\n",
       "       [ 8225],\n",
       "       [ 8224],\n",
       "       [ 8222],\n",
       "       [ 8221],\n",
       "       [ 8220],\n",
       "       [ 8218],\n",
       "       [ 8217],\n",
       "       [ 8216],\n",
       "       [ 8212],\n",
       "       [ 8211],\n",
       "       [  960],\n",
       "       [  733],\n",
       "       [  732],\n",
       "       [  731],\n",
       "       [  730],\n",
       "       [  729],\n",
       "       [  728],\n",
       "       [  713],\n",
       "       [  711],\n",
       "       [  710],\n",
       "       [  402],\n",
       "       [  382],\n",
       "       [  381],\n",
       "       [  376],\n",
       "       [  353],\n",
       "       [  352],\n",
       "       [  339],\n",
       "       [  338],\n",
       "       [  322],\n",
       "       [  321],\n",
       "       [  305],\n",
       "       [  254],\n",
       "       [  253],\n",
       "       [  252],\n",
       "       [  251],\n",
       "       [  250],\n",
       "       [  249],\n",
       "       [  248],\n",
       "       [  247],\n",
       "       [  246],\n",
       "       [  245],\n",
       "       [  244],\n",
       "       [  243],\n",
       "       [  242],\n",
       "       [  241],\n",
       "       [  240],\n",
       "       [  239],\n",
       "       [  238],\n",
       "       [  237],\n",
       "       [  236],\n",
       "       [  235],\n",
       "       [  234],\n",
       "       [  233],\n",
       "       [  232],\n",
       "       [  231],\n",
       "       [  230],\n",
       "       [  229],\n",
       "       [  228],\n",
       "       [  227],\n",
       "       [  226],\n",
       "       [  225],\n",
       "       [  224],\n",
       "       [  223],\n",
       "       [  222],\n",
       "       [  221],\n",
       "       [  220],\n",
       "       [  219],\n",
       "       [  218],\n",
       "       [  217],\n",
       "       [  216],\n",
       "       [  215],\n",
       "       [  214],\n",
       "       [  213],\n",
       "       [  212],\n",
       "       [  211],\n",
       "       [  210],\n",
       "       [  209],\n",
       "       [  208],\n",
       "       [  207],\n",
       "       [  206],\n",
       "       [  205],\n",
       "       [  204],\n",
       "       [  203],\n",
       "       [  202],\n",
       "       [  201],\n",
       "       [  200],\n",
       "       [  199],\n",
       "       [  198],\n",
       "       [  197],\n",
       "       [  196],\n",
       "       [  195],\n",
       "       [  194],\n",
       "       [  193],\n",
       "       [  192],\n",
       "       [  191],\n",
       "       [  190],\n",
       "       [  189],\n",
       "       [  188],\n",
       "       [  187],\n",
       "       [  186],\n",
       "       [  185],\n",
       "       [  184],\n",
       "       [  183],\n",
       "       [  182],\n",
       "       [  181],\n",
       "       [  180],\n",
       "       [  179],\n",
       "       [  178],\n",
       "       [  177],\n",
       "       [  176],\n",
       "       [  175],\n",
       "       [  174],\n",
       "       [  173],\n",
       "       [  172],\n",
       "       [  171],\n",
       "       [  170],\n",
       "       [  169],\n",
       "       [  168],\n",
       "       [  167],\n",
       "       [  166],\n",
       "       [  165],\n",
       "       [  164],\n",
       "       [  163],\n",
       "       [  162],\n",
       "       [  161],\n",
       "       [  126],\n",
       "       [  125],\n",
       "       [  124],\n",
       "       [  123],\n",
       "       [  122],\n",
       "       [  121],\n",
       "       [  120],\n",
       "       [  119],\n",
       "       [  118],\n",
       "       [  117],\n",
       "       [  116],\n",
       "       [  115],\n",
       "       [  114],\n",
       "       [  113],\n",
       "       [  112],\n",
       "       [  111],\n",
       "       [  110],\n",
       "       [  109],\n",
       "       [  108],\n",
       "       [  107],\n",
       "       [  106],\n",
       "       [  105],\n",
       "       [  104],\n",
       "       [  103],\n",
       "       [  102],\n",
       "       [  101],\n",
       "       [  100],\n",
       "       [   99],\n",
       "       [   98],\n",
       "       [   97],\n",
       "       [   96],\n",
       "       [   95],\n",
       "       [   94],\n",
       "       [   93],\n",
       "       [   92],\n",
       "       [   91],\n",
       "       [   90],\n",
       "       [   89],\n",
       "       [   88],\n",
       "       [   87],\n",
       "       [   86],\n",
       "       [   85],\n",
       "       [   84],\n",
       "       [   83],\n",
       "       [   82],\n",
       "       [   81],\n",
       "       [   80],\n",
       "       [   79],\n",
       "       [   78],\n",
       "       [   77],\n",
       "       [   76],\n",
       "       [   75],\n",
       "       [   74],\n",
       "       [   73],\n",
       "       [   72],\n",
       "       [   71],\n",
       "       [   70],\n",
       "       [   69],\n",
       "       [   68],\n",
       "       [   67],\n",
       "       [   66],\n",
       "       [   65],\n",
       "       [   64],\n",
       "       [   63],\n",
       "       [   62],\n",
       "       [   61],\n",
       "       [   60],\n",
       "       [   59],\n",
       "       [   58],\n",
       "       [   57],\n",
       "       [   56],\n",
       "       [   55],\n",
       "       [   54],\n",
       "       [   53],\n",
       "       [   52],\n",
       "       [   51],\n",
       "       [   50],\n",
       "       [   49],\n",
       "       [   48],\n",
       "       [   47],\n",
       "       [   46],\n",
       "       [   45],\n",
       "       [   44],\n",
       "       [   43],\n",
       "       [   42],\n",
       "       [   41],\n",
       "       [   40],\n",
       "       [   39],\n",
       "       [   38],\n",
       "       [   37],\n",
       "       [   36],\n",
       "       [   35],\n",
       "       [   34],\n",
       "       [   33]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for computing the char_to_ix and ix_to_char arrays. \n",
    "# computes those maps given a y vector.\n",
    "def CharToIxArr(Y):\n",
    "    char = np.concatenate(Y)\n",
    "    chars = list(set(char))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(char) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(char) }\n",
    "    char_to_ix = {ch:i for i,ch in enumerate(char_to_ix)}\n",
    "    return char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{61442: 0,\n",
       " 61441: 1,\n",
       " 9674: 2,\n",
       " 8805: 3,\n",
       " 8804: 4,\n",
       " 8800: 5,\n",
       " 8776: 6,\n",
       " 8747: 7,\n",
       " 8734: 8,\n",
       " 8730: 9,\n",
       " 8729: 10,\n",
       " 8725: 11,\n",
       " 8722: 12,\n",
       " 8721: 13,\n",
       " 8719: 14,\n",
       " 8710: 15,\n",
       " 8706: 16,\n",
       " 8486: 17,\n",
       " 8482: 18,\n",
       " 8364: 19,\n",
       " 8250: 20,\n",
       " 8249: 21,\n",
       " 8240: 22,\n",
       " 8230: 23,\n",
       " 8226: 24,\n",
       " 8225: 25,\n",
       " 8224: 26,\n",
       " 8222: 27,\n",
       " 8221: 28,\n",
       " 8220: 29,\n",
       " 8218: 30,\n",
       " 8217: 31,\n",
       " 8216: 32,\n",
       " 8212: 33,\n",
       " 8211: 34,\n",
       " 960: 35,\n",
       " 733: 36,\n",
       " 732: 37,\n",
       " 731: 38,\n",
       " 730: 39,\n",
       " 729: 40,\n",
       " 728: 41,\n",
       " 713: 42,\n",
       " 711: 43,\n",
       " 710: 44,\n",
       " 402: 45,\n",
       " 382: 46,\n",
       " 381: 47,\n",
       " 376: 48,\n",
       " 353: 49,\n",
       " 352: 50,\n",
       " 339: 51,\n",
       " 338: 52,\n",
       " 322: 53,\n",
       " 321: 54,\n",
       " 305: 55,\n",
       " 254: 56,\n",
       " 253: 57,\n",
       " 252: 58,\n",
       " 251: 59,\n",
       " 250: 60,\n",
       " 249: 61,\n",
       " 248: 62,\n",
       " 247: 63,\n",
       " 246: 64,\n",
       " 245: 65,\n",
       " 244: 66,\n",
       " 243: 67,\n",
       " 242: 68,\n",
       " 241: 69,\n",
       " 240: 70,\n",
       " 239: 71,\n",
       " 238: 72,\n",
       " 237: 73,\n",
       " 236: 74,\n",
       " 235: 75,\n",
       " 234: 76,\n",
       " 233: 77,\n",
       " 232: 78,\n",
       " 231: 79,\n",
       " 230: 80,\n",
       " 229: 81,\n",
       " 228: 82,\n",
       " 227: 83,\n",
       " 226: 84,\n",
       " 225: 85,\n",
       " 224: 86,\n",
       " 223: 87,\n",
       " 222: 88,\n",
       " 221: 89,\n",
       " 220: 90,\n",
       " 219: 91,\n",
       " 218: 92,\n",
       " 217: 93,\n",
       " 216: 94,\n",
       " 215: 95,\n",
       " 214: 96,\n",
       " 213: 97,\n",
       " 212: 98,\n",
       " 211: 99,\n",
       " 210: 100,\n",
       " 209: 101,\n",
       " 208: 102,\n",
       " 207: 103,\n",
       " 206: 104,\n",
       " 205: 105,\n",
       " 204: 106,\n",
       " 203: 107,\n",
       " 202: 108,\n",
       " 201: 109,\n",
       " 200: 110,\n",
       " 199: 111,\n",
       " 198: 112,\n",
       " 197: 113,\n",
       " 196: 114,\n",
       " 195: 115,\n",
       " 194: 116,\n",
       " 193: 117,\n",
       " 192: 118,\n",
       " 191: 119,\n",
       " 190: 120,\n",
       " 189: 121,\n",
       " 188: 122,\n",
       " 187: 123,\n",
       " 186: 124,\n",
       " 185: 125,\n",
       " 184: 126,\n",
       " 183: 127,\n",
       " 182: 128,\n",
       " 181: 129,\n",
       " 180: 130,\n",
       " 179: 131,\n",
       " 178: 132,\n",
       " 177: 133,\n",
       " 176: 134,\n",
       " 175: 135,\n",
       " 174: 136,\n",
       " 173: 137,\n",
       " 172: 138,\n",
       " 171: 139,\n",
       " 170: 140,\n",
       " 169: 141,\n",
       " 168: 142,\n",
       " 167: 143,\n",
       " 166: 144,\n",
       " 165: 145,\n",
       " 164: 146,\n",
       " 163: 147,\n",
       " 162: 148,\n",
       " 161: 149,\n",
       " 126: 150,\n",
       " 125: 151,\n",
       " 124: 152,\n",
       " 123: 153,\n",
       " 122: 154,\n",
       " 121: 155,\n",
       " 120: 156,\n",
       " 119: 157,\n",
       " 118: 158,\n",
       " 117: 159,\n",
       " 116: 160,\n",
       " 115: 161,\n",
       " 114: 162,\n",
       " 113: 163,\n",
       " 112: 164,\n",
       " 111: 165,\n",
       " 110: 166,\n",
       " 109: 167,\n",
       " 108: 168,\n",
       " 107: 169,\n",
       " 106: 170,\n",
       " 105: 171,\n",
       " 104: 172,\n",
       " 103: 173,\n",
       " 102: 174,\n",
       " 101: 175,\n",
       " 100: 176,\n",
       " 99: 177,\n",
       " 98: 178,\n",
       " 97: 179,\n",
       " 96: 180,\n",
       " 95: 181,\n",
       " 94: 182,\n",
       " 93: 183,\n",
       " 92: 184,\n",
       " 91: 185,\n",
       " 90: 186,\n",
       " 89: 187,\n",
       " 88: 188,\n",
       " 87: 189,\n",
       " 86: 190,\n",
       " 85: 191,\n",
       " 84: 192,\n",
       " 83: 193,\n",
       " 82: 194,\n",
       " 81: 195,\n",
       " 80: 196,\n",
       " 79: 197,\n",
       " 78: 198,\n",
       " 77: 199,\n",
       " 76: 200,\n",
       " 75: 201,\n",
       " 74: 202,\n",
       " 73: 203,\n",
       " 72: 204,\n",
       " 71: 205,\n",
       " 70: 206,\n",
       " 69: 207,\n",
       " 68: 208,\n",
       " 67: 209,\n",
       " 66: 210,\n",
       " 65: 211,\n",
       " 64: 212,\n",
       " 63: 213,\n",
       " 62: 214,\n",
       " 61: 215,\n",
       " 60: 216,\n",
       " 59: 217,\n",
       " 58: 218,\n",
       " 57: 219,\n",
       " 56: 220,\n",
       " 55: 221,\n",
       " 54: 222,\n",
       " 53: 223,\n",
       " 52: 224,\n",
       " 51: 225,\n",
       " 50: 226,\n",
       " 49: 227,\n",
       " 48: 228,\n",
       " 47: 229,\n",
       " 46: 230,\n",
       " 45: 231,\n",
       " 44: 232,\n",
       " 43: 233,\n",
       " 42: 234,\n",
       " 41: 235,\n",
       " 40: 236,\n",
       " 39: 237,\n",
       " 38: 238,\n",
       " 37: 239,\n",
       " 36: 240,\n",
       " 35: 241,\n",
       " 34: 242,\n",
       " 33: 243}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix, ix_to_char = CharToIxArr(Y_s)\n",
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-hot representation\n",
    "def makeOneHot(char_to_ix, ix_to_char):\n",
    "    shape = (len(ix_to_char), len(char_to_ix))\n",
    "    val = 0\n",
    "    dt = np.int\n",
    "    a = np.empty(shape,dtype=dt)\n",
    "    a.fill(val)\n",
    "    for i in range(len(ix_to_char)):\n",
    "        ch = ix_to_char[i]\n",
    "        a[i][char_to_ix[ch]] = 1\n",
    "    return a\n",
    "a = makeOneHot(char_to_ix, ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "### Junchen helped me extensively with this.\n",
    "def makeModel(chars):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (4,4), input_shape=(20,20, 1)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Conv2D(64, (4,4)),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64),\n",
    "        Activation('relu'),\n",
    "        Dense(len(chars)),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "    # For a multi-class classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "model = makeModel(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "976/976 [==============================] - 1s 582us/step - loss: 5.5062 - acc: 0.0031\n",
      "Epoch 2/100\n",
      "976/976 [==============================] - 0s 280us/step - loss: 5.4415 - acc: 0.0133\n",
      "Epoch 3/100\n",
      "976/976 [==============================] - 0s 256us/step - loss: 5.2392 - acc: 0.0205\n",
      "Epoch 4/100\n",
      "976/976 [==============================] - 0s 254us/step - loss: 4.9361 - acc: 0.0256\n",
      "Epoch 5/100\n",
      "976/976 [==============================] - 0s 256us/step - loss: 4.5553 - acc: 0.0461\n",
      "Epoch 6/100\n",
      "976/976 [==============================] - 0s 260us/step - loss: 4.1102 - acc: 0.0932\n",
      "Epoch 7/100\n",
      "976/976 [==============================] - 0s 272us/step - loss: 3.7185 - acc: 0.1219\n",
      "Epoch 8/100\n",
      "976/976 [==============================] - 0s 298us/step - loss: 3.3698 - acc: 0.1742\n",
      "Epoch 9/100\n",
      "976/976 [==============================] - 0s 272us/step - loss: 3.0620 - acc: 0.2223\n",
      "Epoch 10/100\n",
      "976/976 [==============================] - 0s 296us/step - loss: 2.7879 - acc: 0.2336\n",
      "Epoch 11/100\n",
      "976/976 [==============================] - 0s 333us/step - loss: 2.5199 - acc: 0.3074\n",
      "Epoch 12/100\n",
      "976/976 [==============================] - 0s 329us/step - loss: 2.3352 - acc: 0.3422\n",
      "Epoch 13/100\n",
      "976/976 [==============================] - 0s 327us/step - loss: 2.1263 - acc: 0.3842\n",
      "Epoch 14/100\n",
      "976/976 [==============================] - 0s 258us/step - loss: 1.9509 - acc: 0.4355\n",
      "Epoch 15/100\n",
      "976/976 [==============================] - 0s 255us/step - loss: 1.8611 - acc: 0.4406\n",
      "Epoch 16/100\n",
      "976/976 [==============================] - 0s 273us/step - loss: 1.7124 - acc: 0.4877\n",
      "Epoch 17/100\n",
      "976/976 [==============================] - 0s 254us/step - loss: 1.6123 - acc: 0.5010\n",
      "Epoch 18/100\n",
      "976/976 [==============================] - 0s 253us/step - loss: 1.5138 - acc: 0.5225\n",
      "Epoch 19/100\n",
      "976/976 [==============================] - 0s 259us/step - loss: 1.4055 - acc: 0.5717\n",
      "Epoch 20/100\n",
      "976/976 [==============================] - 0s 271us/step - loss: 1.3692 - acc: 0.5686\n",
      "Epoch 21/100\n",
      "976/976 [==============================] - 0s 264us/step - loss: 1.3174 - acc: 0.5943\n",
      "Epoch 22/100\n",
      "976/976 [==============================] - 0s 258us/step - loss: 1.2285 - acc: 0.5912\n",
      "Epoch 23/100\n",
      "976/976 [==============================] - 0s 256us/step - loss: 1.1709 - acc: 0.6230\n",
      "Epoch 24/100\n",
      "976/976 [==============================] - 0s 268us/step - loss: 1.1336 - acc: 0.6158\n",
      "Epoch 25/100\n",
      "976/976 [==============================] - 0s 267us/step - loss: 1.0892 - acc: 0.6148\n",
      "Epoch 26/100\n",
      "976/976 [==============================] - 0s 271us/step - loss: 1.0278 - acc: 0.6568\n",
      "Epoch 27/100\n",
      "976/976 [==============================] - 0s 260us/step - loss: 1.0044 - acc: 0.6742\n",
      "Epoch 28/100\n",
      "976/976 [==============================] - 0s 274us/step - loss: 0.9448 - acc: 0.6660 0s - loss: 0.8559 - acc: 0.6\n",
      "Epoch 29/100\n",
      "976/976 [==============================] - 0s 259us/step - loss: 0.9621 - acc: 0.6701\n",
      "Epoch 30/100\n",
      "976/976 [==============================] - 0s 264us/step - loss: 0.9371 - acc: 0.6885\n",
      "Epoch 31/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.8962 - acc: 0.6814\n",
      "Epoch 32/100\n",
      "976/976 [==============================] - 0s 262us/step - loss: 0.8716 - acc: 0.6988\n",
      "Epoch 33/100\n",
      "976/976 [==============================] - 0s 249us/step - loss: 0.8534 - acc: 0.7213\n",
      "Epoch 34/100\n",
      "976/976 [==============================] - 0s 249us/step - loss: 0.8095 - acc: 0.7100\n",
      "Epoch 35/100\n",
      "976/976 [==============================] - 0s 264us/step - loss: 0.8031 - acc: 0.7111\n",
      "Epoch 36/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.8050 - acc: 0.7162\n",
      "Epoch 37/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.7497 - acc: 0.7346\n",
      "Epoch 38/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.7140 - acc: 0.7561\n",
      "Epoch 39/100\n",
      "976/976 [==============================] - 0s 251us/step - loss: 0.6831 - acc: 0.7572\n",
      "Epoch 40/100\n",
      "976/976 [==============================] - 0s 253us/step - loss: 0.7257 - acc: 0.7449\n",
      "Epoch 41/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.6780 - acc: 0.7520\n",
      "Epoch 42/100\n",
      "976/976 [==============================] - 0s 249us/step - loss: 0.6558 - acc: 0.7551\n",
      "Epoch 43/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.6770 - acc: 0.7572\n",
      "Epoch 44/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.6308 - acc: 0.7766\n",
      "Epoch 45/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.6156 - acc: 0.7838\n",
      "Epoch 46/100\n",
      "976/976 [==============================] - 0s 249us/step - loss: 0.6336 - acc: 0.7643\n",
      "Epoch 47/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.6285 - acc: 0.7623\n",
      "Epoch 48/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.6437 - acc: 0.7674\n",
      "Epoch 49/100\n",
      "976/976 [==============================] - 0s 253us/step - loss: 0.5747 - acc: 0.7910\n",
      "Epoch 50/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.5675 - acc: 0.7828\n",
      "Epoch 51/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.5484 - acc: 0.7838\n",
      "Epoch 52/100\n",
      "976/976 [==============================] - 0s 253us/step - loss: 0.5767 - acc: 0.7848\n",
      "Epoch 53/100\n",
      "976/976 [==============================] - 0s 257us/step - loss: 0.5695 - acc: 0.7920\n",
      "Epoch 54/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.5393 - acc: 0.7910\n",
      "Epoch 55/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.5214 - acc: 0.8053\n",
      "Epoch 56/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.5613 - acc: 0.7848\n",
      "Epoch 57/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.5024 - acc: 0.8176\n",
      "Epoch 58/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.5291 - acc: 0.8053\n",
      "Epoch 59/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.5016 - acc: 0.8094\n",
      "Epoch 60/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4957 - acc: 0.8186\n",
      "Epoch 61/100\n",
      "976/976 [==============================] - 0s 254us/step - loss: 0.4882 - acc: 0.8033\n",
      "Epoch 62/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4870 - acc: 0.8207\n",
      "Epoch 63/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.5075 - acc: 0.8115\n",
      "Epoch 64/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4543 - acc: 0.8320\n",
      "Epoch 65/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.4504 - acc: 0.8340\n",
      "Epoch 66/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.4474 - acc: 0.8340\n",
      "Epoch 67/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4454 - acc: 0.8309\n",
      "Epoch 68/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.4626 - acc: 0.8268\n",
      "Epoch 69/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.4281 - acc: 0.8422\n",
      "Epoch 70/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.4570 - acc: 0.8238\n",
      "Epoch 71/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4537 - acc: 0.8320\n",
      "Epoch 72/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.4235 - acc: 0.8402\n",
      "Epoch 73/100\n",
      "976/976 [==============================] - 0s 255us/step - loss: 0.4453 - acc: 0.8279\n",
      "Epoch 74/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.4010 - acc: 0.8463\n",
      "Epoch 75/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.4469 - acc: 0.8268\n",
      "Epoch 76/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.4350 - acc: 0.8289\n",
      "Epoch 77/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.3927 - acc: 0.8412\n",
      "Epoch 78/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4042 - acc: 0.8340\n",
      "Epoch 79/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.3981 - acc: 0.8453\n",
      "Epoch 80/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.3970 - acc: 0.8412\n",
      "Epoch 81/100\n",
      "976/976 [==============================] - 0s 251us/step - loss: 0.4050 - acc: 0.8443\n",
      "Epoch 82/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.4032 - acc: 0.8340\n",
      "Epoch 83/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.4003 - acc: 0.8453\n",
      "Epoch 84/100\n",
      "976/976 [==============================] - 0s 244us/step - loss: 0.3791 - acc: 0.8484\n",
      "Epoch 85/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.3642 - acc: 0.8627\n",
      "Epoch 86/100\n",
      "976/976 [==============================] - 0s 252us/step - loss: 0.3863 - acc: 0.8525\n",
      "Epoch 87/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.3770 - acc: 0.8412\n",
      "Epoch 88/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.3788 - acc: 0.8514\n",
      "Epoch 89/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.3991 - acc: 0.8350\n",
      "Epoch 90/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.3557 - acc: 0.8637\n",
      "Epoch 91/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.3951 - acc: 0.8443\n",
      "Epoch 92/100\n",
      "976/976 [==============================] - 0s 247us/step - loss: 0.3442 - acc: 0.8617\n",
      "Epoch 93/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.3569 - acc: 0.8596\n",
      "Epoch 94/100\n",
      "976/976 [==============================] - 0s 244us/step - loss: 0.3590 - acc: 0.8586\n",
      "Epoch 95/100\n",
      "976/976 [==============================] - 0s 244us/step - loss: 0.3750 - acc: 0.8627\n",
      "Epoch 96/100\n",
      "976/976 [==============================] - 0s 245us/step - loss: 0.3387 - acc: 0.8709\n",
      "Epoch 97/100\n",
      "976/976 [==============================] - 0s 250us/step - loss: 0.3680 - acc: 0.8504\n",
      "Epoch 98/100\n",
      "976/976 [==============================] - 0s 248us/step - loss: 0.3307 - acc: 0.8678\n",
      "Epoch 99/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.3283 - acc: 0.8699\n",
      "Epoch 100/100\n",
      "976/976 [==============================] - 0s 246us/step - loss: 0.3536 - acc: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1242aeb70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(np.reshape(X_s,(-1,20,20,1)), a, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, a_train, a_test = train_test_split(X_s, a, train_size = 0.8, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 [==============================] - 0s 576us/step - loss: 5.5088 - acc: 0.0051\n",
      "Epoch 2/100\n",
      "780/780 [==============================] - 0s 251us/step - loss: 5.4757 - acc: 0.0064\n",
      "Epoch 3/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 5.3982 - acc: 0.0115\n",
      "Epoch 4/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 5.1829 - acc: 0.0256\n",
      "Epoch 5/100\n",
      "780/780 [==============================] - 0s 255us/step - loss: 4.9158 - acc: 0.0346\n",
      "Epoch 6/100\n",
      "780/780 [==============================] - 0s 254us/step - loss: 4.5933 - acc: 0.0551\n",
      "Epoch 7/100\n",
      "780/780 [==============================] - 0s 251us/step - loss: 4.1867 - acc: 0.0949\n",
      "Epoch 8/100\n",
      "780/780 [==============================] - 0s 254us/step - loss: 3.8680 - acc: 0.1372\n",
      "Epoch 9/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 3.4936 - acc: 0.1821\n",
      "Epoch 10/100\n",
      "780/780 [==============================] - 0s 254us/step - loss: 3.1619 - acc: 0.2115\n",
      "Epoch 11/100\n",
      "780/780 [==============================] - 0s 258us/step - loss: 2.8454 - acc: 0.2833\n",
      "Epoch 12/100\n",
      "780/780 [==============================] - 0s 250us/step - loss: 2.6579 - acc: 0.3218\n",
      "Epoch 13/100\n",
      "780/780 [==============================] - 0s 248us/step - loss: 2.4110 - acc: 0.3551\n",
      "Epoch 14/100\n",
      "780/780 [==============================] - 0s 260us/step - loss: 2.2143 - acc: 0.4026\n",
      "Epoch 15/100\n",
      "780/780 [==============================] - 0s 253us/step - loss: 2.0325 - acc: 0.4038\n",
      "Epoch 16/100\n",
      "780/780 [==============================] - 0s 256us/step - loss: 1.9115 - acc: 0.4462\n",
      "Epoch 17/100\n",
      "780/780 [==============================] - 0s 247us/step - loss: 1.7793 - acc: 0.4654\n",
      "Epoch 18/100\n",
      "780/780 [==============================] - 0s 256us/step - loss: 1.6591 - acc: 0.5179\n",
      "Epoch 19/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 1.5929 - acc: 0.4974\n",
      "Epoch 20/100\n",
      "780/780 [==============================] - 0s 247us/step - loss: 1.4766 - acc: 0.5615\n",
      "Epoch 21/100\n",
      "780/780 [==============================] - 0s 255us/step - loss: 1.4516 - acc: 0.5679\n",
      "Epoch 22/100\n",
      "780/780 [==============================] - 0s 252us/step - loss: 1.3578 - acc: 0.5692\n",
      "Epoch 23/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 1.3143 - acc: 0.5782\n",
      "Epoch 24/100\n",
      "780/780 [==============================] - 0s 251us/step - loss: 1.2636 - acc: 0.5949\n",
      "Epoch 25/100\n",
      "780/780 [==============================] - 0s 256us/step - loss: 1.1458 - acc: 0.6179\n",
      "Epoch 26/100\n",
      "780/780 [==============================] - 0s 260us/step - loss: 1.1250 - acc: 0.6205\n",
      "Epoch 27/100\n",
      "780/780 [==============================] - 0s 359us/step - loss: 1.1210 - acc: 0.6538\n",
      "Epoch 28/100\n",
      "780/780 [==============================] - 0s 265us/step - loss: 1.0842 - acc: 0.6462\n",
      "Epoch 29/100\n",
      "780/780 [==============================] - 0s 272us/step - loss: 1.0672 - acc: 0.6513\n",
      "Epoch 30/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 0.9241 - acc: 0.6821\n",
      "Epoch 31/100\n",
      "780/780 [==============================] - 0s 271us/step - loss: 0.9585 - acc: 0.6808\n",
      "Epoch 32/100\n",
      "780/780 [==============================] - 0s 253us/step - loss: 0.9197 - acc: 0.7013\n",
      "Epoch 33/100\n",
      "780/780 [==============================] - 0s 259us/step - loss: 0.9090 - acc: 0.6987\n",
      "Epoch 34/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 0.8504 - acc: 0.7115\n",
      "Epoch 35/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 0.8838 - acc: 0.6987\n",
      "Epoch 36/100\n",
      "780/780 [==============================] - 0s 260us/step - loss: 0.8300 - acc: 0.7321\n",
      "Epoch 37/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 0.8040 - acc: 0.7321\n",
      "Epoch 38/100\n",
      "780/780 [==============================] - 0s 268us/step - loss: 0.8160 - acc: 0.7141\n",
      "Epoch 39/100\n",
      "780/780 [==============================] - 0s 296us/step - loss: 0.7828 - acc: 0.7244\n",
      "Epoch 40/100\n",
      "780/780 [==============================] - 0s 267us/step - loss: 0.8156 - acc: 0.7205\n",
      "Epoch 41/100\n",
      "780/780 [==============================] - 0s 305us/step - loss: 0.7218 - acc: 0.7397\n",
      "Epoch 42/100\n",
      "780/780 [==============================] - 0s 284us/step - loss: 0.7309 - acc: 0.7551\n",
      "Epoch 43/100\n",
      "780/780 [==============================] - 0s 291us/step - loss: 0.7228 - acc: 0.7526\n",
      "Epoch 44/100\n",
      "780/780 [==============================] - 0s 301us/step - loss: 0.6843 - acc: 0.7564\n",
      "Epoch 45/100\n",
      "780/780 [==============================] - 0s 267us/step - loss: 0.6577 - acc: 0.7526\n",
      "Epoch 46/100\n",
      "780/780 [==============================] - 0s 290us/step - loss: 0.7097 - acc: 0.7744\n",
      "Epoch 47/100\n",
      "780/780 [==============================] - 0s 315us/step - loss: 0.6255 - acc: 0.7795\n",
      "Epoch 48/100\n",
      "780/780 [==============================] - 0s 323us/step - loss: 0.6555 - acc: 0.7782\n",
      "Epoch 49/100\n",
      "780/780 [==============================] - 0s 263us/step - loss: 0.6634 - acc: 0.7667\n",
      "Epoch 50/100\n",
      "780/780 [==============================] - 0s 256us/step - loss: 0.6308 - acc: 0.7821\n",
      "Epoch 51/100\n",
      "780/780 [==============================] - 0s 261us/step - loss: 0.6194 - acc: 0.7833\n",
      "Epoch 52/100\n",
      "780/780 [==============================] - 0s 266us/step - loss: 0.6439 - acc: 0.7782\n",
      "Epoch 53/100\n",
      "780/780 [==============================] - 0s 263us/step - loss: 0.5566 - acc: 0.8051\n",
      "Epoch 54/100\n",
      "780/780 [==============================] - 0s 254us/step - loss: 0.5616 - acc: 0.8013\n",
      "Epoch 55/100\n",
      "780/780 [==============================] - 0s 260us/step - loss: 0.5585 - acc: 0.8103\n",
      "Epoch 56/100\n",
      "780/780 [==============================] - 0s 255us/step - loss: 0.5783 - acc: 0.7897\n",
      "Epoch 57/100\n",
      "780/780 [==============================] - 0s 252us/step - loss: 0.5721 - acc: 0.7974\n",
      "Epoch 58/100\n",
      "780/780 [==============================] - 0s 255us/step - loss: 0.5497 - acc: 0.8167\n",
      "Epoch 59/100\n",
      "780/780 [==============================] - 0s 255us/step - loss: 0.5364 - acc: 0.8000\n",
      "Epoch 60/100\n",
      "780/780 [==============================] - 0s 269us/step - loss: 0.5182 - acc: 0.8128\n",
      "Epoch 61/100\n",
      "780/780 [==============================] - 0s 299us/step - loss: 0.5559 - acc: 0.8000\n",
      "Epoch 62/100\n",
      "780/780 [==============================] - 0s 262us/step - loss: 0.5192 - acc: 0.7885\n",
      "Epoch 63/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 0.4632 - acc: 0.8321\n",
      "Epoch 64/100\n",
      "780/780 [==============================] - 0s 274us/step - loss: 0.5076 - acc: 0.8218\n",
      "Epoch 65/100\n",
      "780/780 [==============================] - 0s 261us/step - loss: 0.4836 - acc: 0.8231\n",
      "Epoch 66/100\n",
      "780/780 [==============================] - 0s 276us/step - loss: 0.4913 - acc: 0.8192\n",
      "Epoch 67/100\n",
      "780/780 [==============================] - 0s 276us/step - loss: 0.5027 - acc: 0.8179\n",
      "Epoch 68/100\n",
      "780/780 [==============================] - 0s 365us/step - loss: 0.4783 - acc: 0.8205\n",
      "Epoch 69/100\n",
      "780/780 [==============================] - 0s 353us/step - loss: 0.4832 - acc: 0.8282\n",
      "Epoch 70/100\n",
      "780/780 [==============================] - 0s 319us/step - loss: 0.4460 - acc: 0.8462\n",
      "Epoch 71/100\n",
      "780/780 [==============================] - 0s 259us/step - loss: 0.4779 - acc: 0.8256\n",
      "Epoch 72/100\n",
      "780/780 [==============================] - 0s 259us/step - loss: 0.4519 - acc: 0.8269\n",
      "Epoch 73/100\n",
      "780/780 [==============================] - 0s 271us/step - loss: 0.4526 - acc: 0.8308\n",
      "Epoch 74/100\n",
      "780/780 [==============================] - 0s 258us/step - loss: 0.4806 - acc: 0.8218\n",
      "Epoch 75/100\n",
      "780/780 [==============================] - 0s 257us/step - loss: 0.4431 - acc: 0.8436\n",
      "Epoch 76/100\n",
      "780/780 [==============================] - 0s 263us/step - loss: 0.4382 - acc: 0.8474\n",
      "Epoch 77/100\n",
      "780/780 [==============================] - 0s 264us/step - loss: 0.4567 - acc: 0.8295\n",
      "Epoch 78/100\n",
      "780/780 [==============================] - 0s 263us/step - loss: 0.4145 - acc: 0.8513\n",
      "Epoch 79/100\n",
      "780/780 [==============================] - 0s 346us/step - loss: 0.4154 - acc: 0.8333\n",
      "Epoch 80/100\n",
      "780/780 [==============================] - 0s 363us/step - loss: 0.4223 - acc: 0.8359\n",
      "Epoch 81/100\n",
      "780/780 [==============================] - 0s 316us/step - loss: 0.3972 - acc: 0.8449\n",
      "Epoch 82/100\n",
      "780/780 [==============================] - 0s 372us/step - loss: 0.3957 - acc: 0.8500\n",
      "Epoch 83/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.4066 - acc: 0.8538\n",
      "Epoch 84/100\n",
      "780/780 [==============================] - 0s 289us/step - loss: 0.4044 - acc: 0.8423\n",
      "Epoch 85/100\n",
      "780/780 [==============================] - 0s 395us/step - loss: 0.3918 - acc: 0.8474\n",
      "Epoch 86/100\n",
      "780/780 [==============================] - 0s 375us/step - loss: 0.3822 - acc: 0.8538\n",
      "Epoch 87/100\n",
      "780/780 [==============================] - 0s 390us/step - loss: 0.3887 - acc: 0.8474\n",
      "Epoch 88/100\n",
      "780/780 [==============================] - 0s 313us/step - loss: 0.3872 - acc: 0.8641\n",
      "Epoch 89/100\n",
      "780/780 [==============================] - 0s 277us/step - loss: 0.3912 - acc: 0.8564\n",
      "Epoch 90/100\n",
      "780/780 [==============================] - 0s 288us/step - loss: 0.3852 - acc: 0.8474\n",
      "Epoch 91/100\n",
      "780/780 [==============================] - 0s 305us/step - loss: 0.3724 - acc: 0.8564\n",
      "Epoch 92/100\n",
      "780/780 [==============================] - 0s 304us/step - loss: 0.3775 - acc: 0.8564\n",
      "Epoch 93/100\n",
      "780/780 [==============================] - 0s 289us/step - loss: 0.3736 - acc: 0.8590\n",
      "Epoch 94/100\n",
      "780/780 [==============================] - 0s 309us/step - loss: 0.4040 - acc: 0.8449\n",
      "Epoch 95/100\n",
      "780/780 [==============================] - 0s 293us/step - loss: 0.3696 - acc: 0.8500\n",
      "Epoch 96/100\n",
      "780/780 [==============================] - 0s 301us/step - loss: 0.3440 - acc: 0.8705\n",
      "Epoch 97/100\n",
      "780/780 [==============================] - 0s 294us/step - loss: 0.3884 - acc: 0.8551\n",
      "Epoch 98/100\n",
      "780/780 [==============================] - 0s 303us/step - loss: 0.3538 - acc: 0.8641\n",
      "Epoch 99/100\n",
      "780/780 [==============================] - 0s 292us/step - loss: 0.3750 - acc: 0.8615\n",
      "Epoch 100/100\n",
      "780/780 [==============================] - 0s 293us/step - loss: 0.3300 - acc: 0.8679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125233da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make new model and test it using cross validation\n",
    "model2 = makeModel(char_to_ix)\n",
    "model2.fit(np.reshape(x_train, (-1,20,20,1)), a_train, epochs = 100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 0s 427us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.033448649912464, 0.5714285714285714]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(np.reshape(x_test, (-1,20,20,1)), a_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The model is about ranges from about 55% to 62% accurate, averaging around 59% accurate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a different network topology\n",
    "# Try to find a topology that works better than the one described above.\n",
    "def makeImprovedModel(chars):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4,4), input_shape=(20,20, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(SpatialDropout2D(0.5))\n",
    "    model.add(Conv2D(64, (2,2), input_shape=(20,20,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    model.add(SpatialDropout2D(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "    # For a multi-class classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "model3 = makeImprovedModel(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 [==============================] - 1s 881us/step - loss: 5.5118 - acc: 0.0026\n",
      "Epoch 2/100\n",
      "780/780 [==============================] - 0s 457us/step - loss: 5.4705 - acc: 0.0077\n",
      "Epoch 3/100\n",
      "780/780 [==============================] - 0s 380us/step - loss: 5.4282 - acc: 0.0077\n",
      "Epoch 4/100\n",
      "780/780 [==============================] - 0s 440us/step - loss: 5.3489 - acc: 0.0141\n",
      "Epoch 5/100\n",
      "780/780 [==============================] - 0s 450us/step - loss: 5.1697 - acc: 0.0282\n",
      "Epoch 6/100\n",
      "780/780 [==============================] - 0s 405us/step - loss: 4.9245 - acc: 0.0577\n",
      "Epoch 7/100\n",
      "780/780 [==============================] - 0s 396us/step - loss: 4.6111 - acc: 0.0808\n",
      "Epoch 8/100\n",
      "780/780 [==============================] - 0s 467us/step - loss: 4.2389 - acc: 0.1359\n",
      "Epoch 9/100\n",
      "780/780 [==============================] - 0s 389us/step - loss: 3.8599 - acc: 0.1808\n",
      "Epoch 10/100\n",
      "780/780 [==============================] - 0s 354us/step - loss: 3.4125 - acc: 0.2333\n",
      "Epoch 11/100\n",
      "780/780 [==============================] - 0s 377us/step - loss: 3.0686 - acc: 0.2821\n",
      "Epoch 12/100\n",
      "780/780 [==============================] - 0s 371us/step - loss: 2.7837 - acc: 0.3244\n",
      "Epoch 13/100\n",
      "780/780 [==============================] - 0s 365us/step - loss: 2.4532 - acc: 0.3872\n",
      "Epoch 14/100\n",
      "780/780 [==============================] - 0s 371us/step - loss: 2.2439 - acc: 0.4077\n",
      "Epoch 15/100\n",
      "780/780 [==============================] - 0s 502us/step - loss: 2.0253 - acc: 0.4667\n",
      "Epoch 16/100\n",
      "780/780 [==============================] - 0s 348us/step - loss: 1.8417 - acc: 0.5064\n",
      "Epoch 17/100\n",
      "780/780 [==============================] - 0s 408us/step - loss: 1.7179 - acc: 0.5282\n",
      "Epoch 18/100\n",
      "780/780 [==============================] - 0s 420us/step - loss: 1.5981 - acc: 0.5513\n",
      "Epoch 19/100\n",
      "780/780 [==============================] - 0s 379us/step - loss: 1.3817 - acc: 0.6026\n",
      "Epoch 20/100\n",
      "780/780 [==============================] - 0s 391us/step - loss: 1.4195 - acc: 0.5962\n",
      "Epoch 21/100\n",
      "780/780 [==============================] - 0s 385us/step - loss: 1.3051 - acc: 0.6103\n",
      "Epoch 22/100\n",
      "780/780 [==============================] - 0s 370us/step - loss: 1.2219 - acc: 0.6269\n",
      "Epoch 23/100\n",
      "780/780 [==============================] - 0s 399us/step - loss: 1.1863 - acc: 0.6590\n",
      "Epoch 24/100\n",
      "780/780 [==============================] - 0s 380us/step - loss: 1.0467 - acc: 0.6679\n",
      "Epoch 25/100\n",
      "780/780 [==============================] - 0s 431us/step - loss: 1.0584 - acc: 0.6769\n",
      "Epoch 26/100\n",
      "780/780 [==============================] - 0s 594us/step - loss: 0.9684 - acc: 0.7090\n",
      "Epoch 27/100\n",
      "780/780 [==============================] - 0s 544us/step - loss: 0.8867 - acc: 0.7179\n",
      "Epoch 28/100\n",
      "780/780 [==============================] - 0s 396us/step - loss: 0.9417 - acc: 0.6987\n",
      "Epoch 29/100\n",
      "780/780 [==============================] - 0s 466us/step - loss: 0.8703 - acc: 0.7179\n",
      "Epoch 30/100\n",
      "780/780 [==============================] - 0s 464us/step - loss: 0.8785 - acc: 0.7154\n",
      "Epoch 31/100\n",
      "780/780 [==============================] - 0s 430us/step - loss: 0.8059 - acc: 0.7436\n",
      "Epoch 32/100\n",
      "780/780 [==============================] - 0s 345us/step - loss: 0.7988 - acc: 0.7513\n",
      "Epoch 33/100\n",
      "780/780 [==============================] - 0s 419us/step - loss: 0.7345 - acc: 0.7628\n",
      "Epoch 34/100\n",
      "780/780 [==============================] - 0s 388us/step - loss: 0.7183 - acc: 0.7641\n",
      "Epoch 35/100\n",
      "780/780 [==============================] - 0s 361us/step - loss: 0.7279 - acc: 0.7705\n",
      "Epoch 36/100\n",
      "780/780 [==============================] - 0s 356us/step - loss: 0.6691 - acc: 0.7821\n",
      "Epoch 37/100\n",
      "780/780 [==============================] - 0s 346us/step - loss: 0.6668 - acc: 0.7577\n",
      "Epoch 38/100\n",
      "780/780 [==============================] - 0s 344us/step - loss: 0.6142 - acc: 0.8051\n",
      "Epoch 39/100\n",
      "780/780 [==============================] - 0s 334us/step - loss: 0.6179 - acc: 0.8013\n",
      "Epoch 40/100\n",
      "780/780 [==============================] - 0s 356us/step - loss: 0.6156 - acc: 0.7885\n",
      "Epoch 41/100\n",
      "780/780 [==============================] - 0s 349us/step - loss: 0.5952 - acc: 0.7910\n",
      "Epoch 42/100\n",
      "780/780 [==============================] - 0s 343us/step - loss: 0.5709 - acc: 0.8026\n",
      "Epoch 43/100\n",
      "780/780 [==============================] - 0s 352us/step - loss: 0.5679 - acc: 0.8013\n",
      "Epoch 44/100\n",
      "780/780 [==============================] - 0s 341us/step - loss: 0.5424 - acc: 0.8103\n",
      "Epoch 45/100\n",
      "780/780 [==============================] - 0s 337us/step - loss: 0.4974 - acc: 0.8218\n",
      "Epoch 46/100\n",
      "780/780 [==============================] - 0s 334us/step - loss: 0.5159 - acc: 0.8038\n",
      "Epoch 47/100\n",
      "780/780 [==============================] - 0s 336us/step - loss: 0.5422 - acc: 0.8154\n",
      "Epoch 48/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.4731 - acc: 0.8282\n",
      "Epoch 49/100\n",
      "780/780 [==============================] - 0s 336us/step - loss: 0.4909 - acc: 0.8192\n",
      "Epoch 50/100\n",
      "780/780 [==============================] - 0s 333us/step - loss: 0.4306 - acc: 0.8538\n",
      "Epoch 51/100\n",
      "780/780 [==============================] - 0s 332us/step - loss: 0.4832 - acc: 0.8256\n",
      "Epoch 52/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.4481 - acc: 0.8526\n",
      "Epoch 53/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.4548 - acc: 0.8410\n",
      "Epoch 54/100\n",
      "780/780 [==============================] - 0s 336us/step - loss: 0.4394 - acc: 0.8513\n",
      "Epoch 55/100\n",
      "780/780 [==============================] - 0s 334us/step - loss: 0.4449 - acc: 0.8436\n",
      "Epoch 56/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.4151 - acc: 0.8526\n",
      "Epoch 57/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.4336 - acc: 0.8359\n",
      "Epoch 58/100\n",
      "780/780 [==============================] - 0s 334us/step - loss: 0.4321 - acc: 0.8474\n",
      "Epoch 59/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.3934 - acc: 0.8615\n",
      "Epoch 60/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.4073 - acc: 0.8500\n",
      "Epoch 61/100\n",
      "780/780 [==============================] - 0s 350us/step - loss: 0.4137 - acc: 0.8538\n",
      "Epoch 62/100\n",
      "780/780 [==============================] - 0s 342us/step - loss: 0.3997 - acc: 0.8513\n",
      "Epoch 63/100\n",
      "780/780 [==============================] - 0s 350us/step - loss: 0.4124 - acc: 0.8526\n",
      "Epoch 64/100\n",
      "780/780 [==============================] - 0s 333us/step - loss: 0.3904 - acc: 0.8615\n",
      "Epoch 65/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.4137 - acc: 0.8577\n",
      "Epoch 66/100\n",
      "780/780 [==============================] - 0s 367us/step - loss: 0.3814 - acc: 0.8615\n",
      "Epoch 67/100\n",
      "780/780 [==============================] - 0s 399us/step - loss: 0.3907 - acc: 0.8628\n",
      "Epoch 68/100\n",
      "780/780 [==============================] - 0s 365us/step - loss: 0.3814 - acc: 0.8679\n",
      "Epoch 69/100\n",
      "780/780 [==============================] - 0s 338us/step - loss: 0.3895 - acc: 0.8577\n",
      "Epoch 70/100\n",
      "780/780 [==============================] - 0s 361us/step - loss: 0.3810 - acc: 0.8590\n",
      "Epoch 71/100\n",
      "780/780 [==============================] - 0s 361us/step - loss: 0.3757 - acc: 0.8603\n",
      "Epoch 72/100\n",
      "780/780 [==============================] - 0s 374us/step - loss: 0.3625 - acc: 0.8628\n",
      "Epoch 73/100\n",
      "780/780 [==============================] - 0s 391us/step - loss: 0.3487 - acc: 0.8590\n",
      "Epoch 74/100\n",
      "780/780 [==============================] - 0s 337us/step - loss: 0.3552 - acc: 0.8718\n",
      "Epoch 75/100\n",
      "780/780 [==============================] - 0s 332us/step - loss: 0.3355 - acc: 0.8731\n",
      "Epoch 76/100\n",
      "780/780 [==============================] - 0s 327us/step - loss: 0.3637 - acc: 0.8654\n",
      "Epoch 77/100\n",
      "780/780 [==============================] - 0s 336us/step - loss: 0.3377 - acc: 0.8782\n",
      "Epoch 78/100\n",
      "780/780 [==============================] - 0s 364us/step - loss: 0.3430 - acc: 0.8756\n",
      "Epoch 79/100\n",
      "780/780 [==============================] - 0s 347us/step - loss: 0.3211 - acc: 0.8821\n",
      "Epoch 80/100\n",
      "780/780 [==============================] - 0s 351us/step - loss: 0.3149 - acc: 0.8692\n",
      "Epoch 81/100\n",
      "780/780 [==============================] - 0s 339us/step - loss: 0.3081 - acc: 0.8885\n",
      "Epoch 82/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.3420 - acc: 0.8756\n",
      "Epoch 83/100\n",
      "780/780 [==============================] - 0s 331us/step - loss: 0.3275 - acc: 0.8795\n",
      "Epoch 84/100\n",
      "780/780 [==============================] - 0s 335us/step - loss: 0.3304 - acc: 0.8833\n",
      "Epoch 85/100\n",
      "780/780 [==============================] - 0s 353us/step - loss: 0.3371 - acc: 0.8731\n",
      "Epoch 86/100\n",
      "780/780 [==============================] - 0s 377us/step - loss: 0.3383 - acc: 0.8731\n",
      "Epoch 87/100\n",
      "780/780 [==============================] - 0s 374us/step - loss: 0.2913 - acc: 0.8910\n",
      "Epoch 88/100\n",
      "780/780 [==============================] - 0s 366us/step - loss: 0.2948 - acc: 0.8885\n",
      "Epoch 89/100\n",
      "780/780 [==============================] - 0s 362us/step - loss: 0.3183 - acc: 0.8885\n",
      "Epoch 90/100\n",
      "780/780 [==============================] - 0s 372us/step - loss: 0.3290 - acc: 0.8679\n",
      "Epoch 91/100\n",
      "780/780 [==============================] - 0s 366us/step - loss: 0.2957 - acc: 0.8833\n",
      "Epoch 92/100\n",
      "780/780 [==============================] - 0s 358us/step - loss: 0.3108 - acc: 0.8756\n",
      "Epoch 93/100\n",
      "780/780 [==============================] - 0s 438us/step - loss: 0.2728 - acc: 0.9000\n",
      "Epoch 94/100\n",
      "780/780 [==============================] - 0s 637us/step - loss: 0.3172 - acc: 0.8782\n",
      "Epoch 95/100\n",
      "780/780 [==============================] - 0s 374us/step - loss: 0.2805 - acc: 0.8885\n",
      "Epoch 96/100\n",
      "780/780 [==============================] - 0s 330us/step - loss: 0.2829 - acc: 0.8872\n",
      "Epoch 97/100\n",
      "780/780 [==============================] - 0s 334us/step - loss: 0.3086 - acc: 0.8859\n",
      "Epoch 98/100\n",
      "780/780 [==============================] - 0s 328us/step - loss: 0.2852 - acc: 0.8949\n",
      "Epoch 99/100\n",
      "780/780 [==============================] - 0s 327us/step - loss: 0.2618 - acc: 0.9051\n",
      "Epoch 100/100\n",
      "780/780 [==============================] - 0s 317us/step - loss: 0.2867 - acc: 0.8859\n",
      "196/196 [==============================] - 0s 536us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.747477234626303, 0.6581632653061225]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.reshape(x_train, (-1,20,20,1)), a_train, epochs = 100, batch_size=32)\n",
    "model3.evaluate(np.reshape(x_test, (-1,20,20,1)), a_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I was able to get 65-69% accuracy (averaging 67%, about 8% better) by increasing the number of layers, changing the second convolution layer, increasing the initial dropout from .4 to .5, reducing the pool size of the second maxpool. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***New Data Set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf2 = makeDataFrame(path, \"BANKGOTHIC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_s, Y2_s = dfTransform(nf2)\n",
    "char2_to_ix2, ix2_to_char2 = CharToIxArr(Y2_s)\n",
    "b = makeOneHot(char2_to_ix2, ix2_to_char2)\n",
    "x2_train, x2_test, b_train, b_test = train_test_split(X2_s, b, train_size = 0.8, test_size = 0.2)\n",
    "model4 = makeImprovedModel(char2_to_ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1792/1792 [==============================] - 1s 536us/step - loss: 5.6352 - acc: 0.0061\n",
      "Epoch 2/100\n",
      "1792/1792 [==============================] - 1s 314us/step - loss: 5.4920 - acc: 0.0173\n",
      "Epoch 3/100\n",
      "1792/1792 [==============================] - 1s 319us/step - loss: 4.8109 - acc: 0.0586\n",
      "Epoch 4/100\n",
      "1792/1792 [==============================] - 1s 324us/step - loss: 3.7394 - acc: 0.1596\n",
      "Epoch 5/100\n",
      "1792/1792 [==============================] - 1s 319us/step - loss: 2.8296 - acc: 0.2701\n",
      "Epoch 6/100\n",
      "1792/1792 [==============================] - 1s 319us/step - loss: 2.3833 - acc: 0.3588\n",
      "Epoch 7/100\n",
      "1792/1792 [==============================] - 1s 348us/step - loss: 2.0151 - acc: 0.4196\n",
      "Epoch 8/100\n",
      "1792/1792 [==============================] - 1s 320us/step - loss: 1.8846 - acc: 0.4392\n",
      "Epoch 9/100\n",
      "1792/1792 [==============================] - 1s 314us/step - loss: 1.6610 - acc: 0.5017\n",
      "Epoch 10/100\n",
      "1792/1792 [==============================] - 1s 319us/step - loss: 1.5956 - acc: 0.5195\n",
      "Epoch 11/100\n",
      "1792/1792 [==============================] - 1s 317us/step - loss: 1.5307 - acc: 0.5223\n",
      "Epoch 12/100\n",
      "1792/1792 [==============================] - 1s 351us/step - loss: 1.4303 - acc: 0.5469\n",
      "Epoch 13/100\n",
      "1792/1792 [==============================] - 1s 396us/step - loss: 1.3738 - acc: 0.5653\n",
      "Epoch 14/100\n",
      "1792/1792 [==============================] - 1s 324us/step - loss: 1.3130 - acc: 0.5709\n",
      "Epoch 15/100\n",
      "1792/1792 [==============================] - 1s 339us/step - loss: 1.2667 - acc: 0.5887\n",
      "Epoch 16/100\n",
      "1792/1792 [==============================] - 1s 315us/step - loss: 1.2283 - acc: 0.5915\n",
      "Epoch 17/100\n",
      "1792/1792 [==============================] - 1s 319us/step - loss: 1.1709 - acc: 0.6250\n",
      "Epoch 18/100\n",
      "1792/1792 [==============================] - 1s 313us/step - loss: 1.1897 - acc: 0.6211\n",
      "Epoch 19/100\n",
      "1792/1792 [==============================] - 1s 321us/step - loss: 1.1304 - acc: 0.6233\n",
      "Epoch 20/100\n",
      "1792/1792 [==============================] - 1s 309us/step - loss: 1.1142 - acc: 0.6189\n",
      "Epoch 21/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 1.0752 - acc: 0.6350\n",
      "Epoch 22/100\n",
      "1792/1792 [==============================] - 1s 332us/step - loss: 1.0792 - acc: 0.6328\n",
      "Epoch 23/100\n",
      "1792/1792 [==============================] - 1s 352us/step - loss: 1.0321 - acc: 0.6568\n",
      "Epoch 24/100\n",
      "1792/1792 [==============================] - 1s 343us/step - loss: 1.0308 - acc: 0.6496\n",
      "Epoch 25/100\n",
      "1792/1792 [==============================] - 1s 349us/step - loss: 1.0119 - acc: 0.6501\n",
      "Epoch 26/100\n",
      "1792/1792 [==============================] - 1s 329us/step - loss: 0.9965 - acc: 0.6613\n",
      "Epoch 27/100\n",
      "1792/1792 [==============================] - 1s 310us/step - loss: 0.9937 - acc: 0.6613\n",
      "Epoch 28/100\n",
      "1792/1792 [==============================] - 1s 297us/step - loss: 0.9707 - acc: 0.6629\n",
      "Epoch 29/100\n",
      "1792/1792 [==============================] - 1s 292us/step - loss: 0.9512 - acc: 0.6724\n",
      "Epoch 30/100\n",
      "1792/1792 [==============================] - 1s 293us/step - loss: 0.9334 - acc: 0.6680\n",
      "Epoch 31/100\n",
      "1792/1792 [==============================] - 1s 300us/step - loss: 0.9418 - acc: 0.6713\n",
      "Epoch 32/100\n",
      "1792/1792 [==============================] - 1s 293us/step - loss: 0.9137 - acc: 0.6769\n",
      "Epoch 33/100\n",
      "1792/1792 [==============================] - 1s 293us/step - loss: 0.9078 - acc: 0.6847\n",
      "Epoch 34/100\n",
      "1792/1792 [==============================] - 1s 294us/step - loss: 0.9056 - acc: 0.6825\n",
      "Epoch 35/100\n",
      "1792/1792 [==============================] - 1s 299us/step - loss: 0.8805 - acc: 0.6914\n",
      "Epoch 36/100\n",
      "1792/1792 [==============================] - 1s 309us/step - loss: 0.8864 - acc: 0.6853\n",
      "Epoch 37/100\n",
      "1792/1792 [==============================] - 1s 318us/step - loss: 0.8694 - acc: 0.6908\n",
      "Epoch 38/100\n",
      "1792/1792 [==============================] - 1s 306us/step - loss: 0.8523 - acc: 0.6981\n",
      "Epoch 39/100\n",
      "1792/1792 [==============================] - 1s 372us/step - loss: 0.8658 - acc: 0.6953\n",
      "Epoch 40/100\n",
      "1792/1792 [==============================] - 1s 313us/step - loss: 0.8239 - acc: 0.7087\n",
      "Epoch 41/100\n",
      "1792/1792 [==============================] - 1s 299us/step - loss: 0.8408 - acc: 0.7015\n",
      "Epoch 42/100\n",
      "1792/1792 [==============================] - 1s 307us/step - loss: 0.8414 - acc: 0.7076\n",
      "Epoch 43/100\n",
      "1792/1792 [==============================] - 1s 303us/step - loss: 0.8202 - acc: 0.7048\n",
      "Epoch 44/100\n",
      "1792/1792 [==============================] - 1s 304us/step - loss: 0.8330 - acc: 0.6964\n",
      "Epoch 45/100\n",
      "1792/1792 [==============================] - 1s 299us/step - loss: 0.8391 - acc: 0.7037\n",
      "Epoch 46/100\n",
      "1792/1792 [==============================] - 1s 306us/step - loss: 0.8117 - acc: 0.7037\n",
      "Epoch 47/100\n",
      "1792/1792 [==============================] - 1s 301us/step - loss: 0.7928 - acc: 0.7154\n",
      "Epoch 48/100\n",
      "1792/1792 [==============================] - 1s 303us/step - loss: 0.8214 - acc: 0.7048\n",
      "Epoch 49/100\n",
      "1792/1792 [==============================] - 1s 308us/step - loss: 0.8125 - acc: 0.7065\n",
      "Epoch 50/100\n",
      "1792/1792 [==============================] - 1s 325us/step - loss: 0.8038 - acc: 0.7193\n",
      "Epoch 51/100\n",
      "1792/1792 [==============================] - 1s 308us/step - loss: 0.8031 - acc: 0.7193\n",
      "Epoch 52/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 0.7896 - acc: 0.7215\n",
      "Epoch 53/100\n",
      "1792/1792 [==============================] - 1s 309us/step - loss: 0.7858 - acc: 0.7215\n",
      "Epoch 54/100\n",
      "1792/1792 [==============================] - 1s 311us/step - loss: 0.7757 - acc: 0.7193\n",
      "Epoch 55/100\n",
      "1792/1792 [==============================] - 1s 317us/step - loss: 0.7642 - acc: 0.7338\n",
      "Epoch 56/100\n",
      "1792/1792 [==============================] - 1s 310us/step - loss: 0.7682 - acc: 0.7254\n",
      "Epoch 57/100\n",
      "1792/1792 [==============================] - 1s 308us/step - loss: 0.7863 - acc: 0.7160\n",
      "Epoch 58/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 0.7656 - acc: 0.7260\n",
      "Epoch 59/100\n",
      "1792/1792 [==============================] - 1s 309us/step - loss: 0.7466 - acc: 0.7221\n",
      "Epoch 60/100\n",
      "1792/1792 [==============================] - 1s 310us/step - loss: 0.7470 - acc: 0.7338\n",
      "Epoch 61/100\n",
      "1792/1792 [==============================] - 1s 313us/step - loss: 0.7608 - acc: 0.7243\n",
      "Epoch 62/100\n",
      "1792/1792 [==============================] - 1s 310us/step - loss: 0.7351 - acc: 0.7355\n",
      "Epoch 63/100\n",
      "1792/1792 [==============================] - 1s 308us/step - loss: 0.7511 - acc: 0.7288\n",
      "Epoch 64/100\n",
      "1792/1792 [==============================] - 1s 313us/step - loss: 0.7441 - acc: 0.7288\n",
      "Epoch 65/100\n",
      "1792/1792 [==============================] - 1s 311us/step - loss: 0.7577 - acc: 0.7294\n",
      "Epoch 66/100\n",
      "1792/1792 [==============================] - 1s 316us/step - loss: 0.7408 - acc: 0.7316\n",
      "Epoch 67/100\n",
      "1792/1792 [==============================] - 1s 315us/step - loss: 0.7194 - acc: 0.7427\n",
      "Epoch 68/100\n",
      "1792/1792 [==============================] - 1s 313us/step - loss: 0.7356 - acc: 0.7338\n",
      "Epoch 69/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 0.7428 - acc: 0.7377\n",
      "Epoch 70/100\n",
      "1792/1792 [==============================] - 1s 320us/step - loss: 0.7328 - acc: 0.7388\n",
      "Epoch 71/100\n",
      "1792/1792 [==============================] - 1s 325us/step - loss: 0.7209 - acc: 0.7400\n",
      "Epoch 72/100\n",
      "1792/1792 [==============================] - 1s 321us/step - loss: 0.7209 - acc: 0.7383\n",
      "Epoch 73/100\n",
      "1792/1792 [==============================] - 1s 371us/step - loss: 0.7308 - acc: 0.7377\n",
      "Epoch 74/100\n",
      "1792/1792 [==============================] - 1s 393us/step - loss: 0.7242 - acc: 0.7349\n",
      "Epoch 75/100\n",
      "1792/1792 [==============================] - 1s 347us/step - loss: 0.7214 - acc: 0.7494\n",
      "Epoch 76/100\n",
      "1792/1792 [==============================] - 1s 334us/step - loss: 0.7273 - acc: 0.7388\n",
      "Epoch 77/100\n",
      "1792/1792 [==============================] - 1s 331us/step - loss: 0.7200 - acc: 0.7383\n",
      "Epoch 78/100\n",
      "1792/1792 [==============================] - 1s 324us/step - loss: 0.7139 - acc: 0.7467\n",
      "Epoch 79/100\n",
      "1792/1792 [==============================] - 1s 324us/step - loss: 0.7053 - acc: 0.7461\n",
      "Epoch 80/100\n",
      "1792/1792 [==============================] - 1s 329us/step - loss: 0.7041 - acc: 0.7444\n",
      "Epoch 81/100\n",
      "1792/1792 [==============================] - 1s 326us/step - loss: 0.7056 - acc: 0.7366\n",
      "Epoch 82/100\n",
      "1792/1792 [==============================] - 1s 309us/step - loss: 0.7041 - acc: 0.7405\n",
      "Epoch 83/100\n",
      "1792/1792 [==============================] - 1s 330us/step - loss: 0.6997 - acc: 0.7478\n",
      "Epoch 84/100\n",
      "1792/1792 [==============================] - 1s 339us/step - loss: 0.7086 - acc: 0.7416\n",
      "Epoch 85/100\n",
      "1792/1792 [==============================] - 1s 401us/step - loss: 0.6987 - acc: 0.7383\n",
      "Epoch 86/100\n",
      "1792/1792 [==============================] - 1s 396us/step - loss: 0.6895 - acc: 0.7533\n",
      "Epoch 87/100\n",
      "1792/1792 [==============================] - 1s 329us/step - loss: 0.6918 - acc: 0.7617\n",
      "Epoch 88/100\n",
      "1792/1792 [==============================] - 1s 314us/step - loss: 0.7004 - acc: 0.7533\n",
      "Epoch 89/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 0.6881 - acc: 0.7506\n",
      "Epoch 90/100\n",
      "1792/1792 [==============================] - 1s 314us/step - loss: 0.6854 - acc: 0.7467\n",
      "Epoch 91/100\n",
      "1792/1792 [==============================] - 1s 312us/step - loss: 0.6993 - acc: 0.7511\n",
      "Epoch 92/100\n",
      "1792/1792 [==============================] - 1s 310us/step - loss: 0.6724 - acc: 0.7634\n",
      "Epoch 93/100\n",
      "1792/1792 [==============================] - 1s 311us/step - loss: 0.6909 - acc: 0.7528\n",
      "Epoch 94/100\n",
      "1792/1792 [==============================] - 1s 316us/step - loss: 0.6879 - acc: 0.7511\n",
      "Epoch 95/100\n",
      "1792/1792 [==============================] - 1s 315us/step - loss: 0.6757 - acc: 0.7545\n",
      "Epoch 96/100\n",
      "1792/1792 [==============================] - 1s 326us/step - loss: 0.6641 - acc: 0.7656\n",
      "Epoch 97/100\n",
      "1792/1792 [==============================] - 1s 322us/step - loss: 0.6808 - acc: 0.7573\n",
      "Epoch 98/100\n",
      "1792/1792 [==============================] - 1s 335us/step - loss: 0.6666 - acc: 0.7640\n",
      "Epoch 99/100\n",
      "1792/1792 [==============================] - 1s 315us/step - loss: 0.6716 - acc: 0.7623\n",
      "Epoch 100/100\n",
      "1792/1792 [==============================] - 1s 318us/step - loss: 0.6669 - acc: 0.7595\n",
      "448/448 [==============================] - 0s 321us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8403047748974392, 0.6785714285714286]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(np.reshape(x2_train, (-1,20,20,1)), b_train, epochs = 100, batch_size=32)\n",
    "model4.evaluate(np.reshape(x2_test, (-1,20,20,1)), b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***My network got 61-68% accuracy when trained and tested on BANKGOTHIC, and 18% accuracy when trained on TAHOMA. I had to re-train and retest because testing the model on a new data set doesn't always seem to work. They have differing amounts of unique characters. I believe the difference in accuracy is because TAHOMA has over 3000 characters and BANKGOTHIC has around 300. (similar to KUNSTLER, which I started wtih)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with three fonts\n",
    "nf3 = makeDataFrame(path, \"B*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_s, Y3_s = dfTransform(nf3)\n",
    "char3_to_ix3, ix3_to_char3 = CharToIxArr(Y3_s)\n",
    "c = makeOneHot(char3_to_ix3, ix3_to_char3)\n",
    "x3_train, x3_test, c_train, c_test = train_test_split(X3_s, c, train_size = 0.8, test_size = 0.2)\n",
    "model5 = makeImprovedModel(char3_to_ix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3340/3340 [==============================] - 2s 452us/step - loss: 5.6378 - acc: 0.0066\n",
      "Epoch 2/100\n",
      "3340/3340 [==============================] - 1s 363us/step - loss: 5.2407 - acc: 0.0407\n",
      "Epoch 3/100\n",
      "3340/3340 [==============================] - 1s 336us/step - loss: 4.0046 - acc: 0.1614\n",
      "Epoch 4/100\n",
      "3340/3340 [==============================] - 1s 328us/step - loss: 3.0716 - acc: 0.2743\n",
      "Epoch 5/100\n",
      "3340/3340 [==============================] - 1s 316us/step - loss: 2.5295 - acc: 0.3722\n",
      "Epoch 6/100\n",
      "3340/3340 [==============================] - 1s 316us/step - loss: 2.2166 - acc: 0.4198\n",
      "Epoch 7/100\n",
      "3340/3340 [==============================] - 1s 323us/step - loss: 1.9842 - acc: 0.4584\n",
      "Epoch 8/100\n",
      "3340/3340 [==============================] - 1s 328us/step - loss: 1.8207 - acc: 0.4907\n",
      "Epoch 9/100\n",
      "3340/3340 [==============================] - 1s 337us/step - loss: 1.6865 - acc: 0.5177\n",
      "Epoch 10/100\n",
      "3340/3340 [==============================] - 1s 327us/step - loss: 1.5853 - acc: 0.5344\n",
      "Epoch 11/100\n",
      "3340/3340 [==============================] - 1s 329us/step - loss: 1.4850 - acc: 0.5527\n",
      "Epoch 12/100\n",
      "3340/3340 [==============================] - 1s 319us/step - loss: 1.3866 - acc: 0.5790\n",
      "Epoch 13/100\n",
      "3340/3340 [==============================] - 1s 327us/step - loss: 1.3069 - acc: 0.5985\n",
      "Epoch 14/100\n",
      "3340/3340 [==============================] - 1s 379us/step - loss: 1.2590 - acc: 0.6036\n",
      "Epoch 15/100\n",
      "3340/3340 [==============================] - 1s 344us/step - loss: 1.1903 - acc: 0.6219\n",
      "Epoch 16/100\n",
      "3340/3340 [==============================] - 1s 335us/step - loss: 1.1396 - acc: 0.6368\n",
      "Epoch 17/100\n",
      "3340/3340 [==============================] - 1s 331us/step - loss: 1.0933 - acc: 0.6461\n",
      "Epoch 18/100\n",
      "3340/3340 [==============================] - 1s 330us/step - loss: 1.0499 - acc: 0.6710\n",
      "Epoch 19/100\n",
      "3340/3340 [==============================] - 1s 340us/step - loss: 1.0307 - acc: 0.6662\n",
      "Epoch 20/100\n",
      "3340/3340 [==============================] - 1s 381us/step - loss: 1.0020 - acc: 0.6814\n",
      "Epoch 21/100\n",
      "3340/3340 [==============================] - 1s 346us/step - loss: 1.0041 - acc: 0.6781\n",
      "Epoch 22/100\n",
      "3340/3340 [==============================] - 1s 324us/step - loss: 0.9507 - acc: 0.6943\n",
      "Epoch 23/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.9287 - acc: 0.6955\n",
      "Epoch 24/100\n",
      "3340/3340 [==============================] - 1s 321us/step - loss: 0.9273 - acc: 0.6949\n",
      "Epoch 25/100\n",
      "3340/3340 [==============================] - 1s 325us/step - loss: 0.8740 - acc: 0.7183\n",
      "Epoch 26/100\n",
      "3340/3340 [==============================] - 1s 320us/step - loss: 0.8800 - acc: 0.7114\n",
      "Epoch 27/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.8487 - acc: 0.7275\n",
      "Epoch 28/100\n",
      "3340/3340 [==============================] - 1s 320us/step - loss: 0.8635 - acc: 0.7123\n",
      "Epoch 29/100\n",
      "3340/3340 [==============================] - 1s 324us/step - loss: 0.8358 - acc: 0.7219\n",
      "Epoch 30/100\n",
      "3340/3340 [==============================] - 1s 321us/step - loss: 0.8002 - acc: 0.7416\n",
      "Epoch 31/100\n",
      "3340/3340 [==============================] - 1s 323us/step - loss: 0.8127 - acc: 0.7359\n",
      "Epoch 32/100\n",
      "3340/3340 [==============================] - 1s 321us/step - loss: 0.8068 - acc: 0.7344\n",
      "Epoch 33/100\n",
      "3340/3340 [==============================] - 1s 323us/step - loss: 0.7762 - acc: 0.7416\n",
      "Epoch 34/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.7707 - acc: 0.7530\n",
      "Epoch 35/100\n",
      "3340/3340 [==============================] - 1s 324us/step - loss: 0.7616 - acc: 0.7494\n",
      "Epoch 36/100\n",
      "3340/3340 [==============================] - 1s 320us/step - loss: 0.7532 - acc: 0.7467\n",
      "Epoch 37/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.7580 - acc: 0.7434\n",
      "Epoch 38/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.7318 - acc: 0.7533\n",
      "Epoch 39/100\n",
      "3340/3340 [==============================] - 1s 326us/step - loss: 0.7085 - acc: 0.7638\n",
      "Epoch 40/100\n",
      "3340/3340 [==============================] - 1s 350us/step - loss: 0.7119 - acc: 0.7662\n",
      "Epoch 41/100\n",
      "3340/3340 [==============================] - 1s 375us/step - loss: 0.7318 - acc: 0.7566\n",
      "Epoch 42/100\n",
      "3340/3340 [==============================] - 1s 362us/step - loss: 0.7159 - acc: 0.7713\n",
      "Epoch 43/100\n",
      "3340/3340 [==============================] - 1s 346us/step - loss: 0.7038 - acc: 0.7605\n",
      "Epoch 44/100\n",
      "3340/3340 [==============================] - 1s 346us/step - loss: 0.6845 - acc: 0.7725\n",
      "Epoch 45/100\n",
      "3340/3340 [==============================] - 1s 348us/step - loss: 0.7014 - acc: 0.7677\n",
      "Epoch 46/100\n",
      "3340/3340 [==============================] - 1s 411us/step - loss: 0.6816 - acc: 0.7671\n",
      "Epoch 47/100\n",
      "3340/3340 [==============================] - 1s 353us/step - loss: 0.6560 - acc: 0.7805\n",
      "Epoch 48/100\n",
      "3340/3340 [==============================] - 1s 362us/step - loss: 0.6848 - acc: 0.7659\n",
      "Epoch 49/100\n",
      "3340/3340 [==============================] - 1s 327us/step - loss: 0.6730 - acc: 0.7704\n",
      "Epoch 50/100\n",
      "3340/3340 [==============================] - 1s 321us/step - loss: 0.6593 - acc: 0.7781\n",
      "Epoch 51/100\n",
      "3340/3340 [==============================] - 1s 322us/step - loss: 0.6700 - acc: 0.7737\n",
      "Epoch 52/100\n",
      "3340/3340 [==============================] - 1s 320us/step - loss: 0.6702 - acc: 0.7835\n",
      "Epoch 53/100\n",
      "3340/3340 [==============================] - 1s 319us/step - loss: 0.6418 - acc: 0.7805\n",
      "Epoch 54/100\n",
      "3340/3340 [==============================] - 1s 329us/step - loss: 0.6481 - acc: 0.7817\n",
      "Epoch 55/100\n",
      "3340/3340 [==============================] - 1s 383us/step - loss: 0.6445 - acc: 0.7754\n",
      "Epoch 56/100\n",
      "3340/3340 [==============================] - 1s 380us/step - loss: 0.6406 - acc: 0.7856\n",
      "Epoch 57/100\n",
      "3340/3340 [==============================] - 1s 372us/step - loss: 0.6209 - acc: 0.7880\n",
      "Epoch 58/100\n",
      "3340/3340 [==============================] - 2s 467us/step - loss: 0.6416 - acc: 0.7856\n",
      "Epoch 59/100\n",
      "3340/3340 [==============================] - 1s 418us/step - loss: 0.6142 - acc: 0.7901\n",
      "Epoch 60/100\n",
      "3340/3340 [==============================] - 1s 421us/step - loss: 0.6216 - acc: 0.7895\n",
      "Epoch 61/100\n",
      "3340/3340 [==============================] - 1s 404us/step - loss: 0.6169 - acc: 0.7865\n",
      "Epoch 62/100\n",
      "3340/3340 [==============================] - 1s 438us/step - loss: 0.6080 - acc: 0.7874\n",
      "Epoch 63/100\n",
      "3340/3340 [==============================] - 1s 404us/step - loss: 0.6328 - acc: 0.7862\n",
      "Epoch 64/100\n",
      "3340/3340 [==============================] - 1s 389us/step - loss: 0.6025 - acc: 0.7901\n",
      "Epoch 65/100\n",
      "3340/3340 [==============================] - 1s 386us/step - loss: 0.6044 - acc: 0.7916\n",
      "Epoch 66/100\n",
      "3340/3340 [==============================] - 1s 389us/step - loss: 0.6103 - acc: 0.7949\n",
      "Epoch 67/100\n",
      "3340/3340 [==============================] - 1s 447us/step - loss: 0.6014 - acc: 0.7967\n",
      "Epoch 68/100\n",
      "3340/3340 [==============================] - 1s 343us/step - loss: 0.5935 - acc: 0.8018\n",
      "Epoch 69/100\n",
      "3340/3340 [==============================] - 1s 422us/step - loss: 0.5878 - acc: 0.8006\n",
      "Epoch 70/100\n",
      "3340/3340 [==============================] - 1s 408us/step - loss: 0.5813 - acc: 0.8006\n",
      "Epoch 71/100\n",
      "3340/3340 [==============================] - 2s 486us/step - loss: 0.5911 - acc: 0.7979\n",
      "Epoch 72/100\n",
      "3340/3340 [==============================] - 1s 379us/step - loss: 0.5894 - acc: 0.8027\n",
      "Epoch 73/100\n",
      "3340/3340 [==============================] - 1s 364us/step - loss: 0.5887 - acc: 0.8021\n",
      "Epoch 74/100\n",
      "3340/3340 [==============================] - 1s 346us/step - loss: 0.5685 - acc: 0.8117\n",
      "Epoch 75/100\n",
      "3340/3340 [==============================] - 1s 339us/step - loss: 0.5801 - acc: 0.8045\n",
      "Epoch 76/100\n",
      "3340/3340 [==============================] - 1s 350us/step - loss: 0.5632 - acc: 0.8096\n",
      "Epoch 77/100\n",
      "3340/3340 [==============================] - 1s 345us/step - loss: 0.5615 - acc: 0.8048\n",
      "Epoch 78/100\n",
      "3340/3340 [==============================] - 1s 349us/step - loss: 0.5788 - acc: 0.7988\n",
      "Epoch 79/100\n",
      "3340/3340 [==============================] - 1s 340us/step - loss: 0.5836 - acc: 0.7991\n",
      "Epoch 80/100\n",
      "3340/3340 [==============================] - 1s 332us/step - loss: 0.5645 - acc: 0.8108\n",
      "Epoch 81/100\n",
      "3340/3340 [==============================] - 1s 354us/step - loss: 0.5523 - acc: 0.8156\n",
      "Epoch 82/100\n",
      "3340/3340 [==============================] - 1s 342us/step - loss: 0.5653 - acc: 0.8102\n",
      "Epoch 83/100\n",
      "3340/3340 [==============================] - 1s 337us/step - loss: 0.5643 - acc: 0.8108\n",
      "Epoch 84/100\n",
      "3340/3340 [==============================] - 1s 336us/step - loss: 0.5711 - acc: 0.8057\n",
      "Epoch 85/100\n",
      "3340/3340 [==============================] - 1s 335us/step - loss: 0.5619 - acc: 0.8129\n",
      "Epoch 86/100\n",
      "3340/3340 [==============================] - 1s 332us/step - loss: 0.5585 - acc: 0.8051\n",
      "Epoch 87/100\n",
      "3340/3340 [==============================] - 1s 334us/step - loss: 0.5479 - acc: 0.8156\n",
      "Epoch 88/100\n",
      "3340/3340 [==============================] - 1s 334us/step - loss: 0.5602 - acc: 0.8102\n",
      "Epoch 89/100\n",
      "3340/3340 [==============================] - 1s 332us/step - loss: 0.5559 - acc: 0.8141\n",
      "Epoch 90/100\n",
      "3340/3340 [==============================] - 1s 336us/step - loss: 0.5571 - acc: 0.8117\n",
      "Epoch 91/100\n",
      "3340/3340 [==============================] - 1s 337us/step - loss: 0.5509 - acc: 0.8048\n",
      "Epoch 92/100\n",
      "3340/3340 [==============================] - 1s 333us/step - loss: 0.5589 - acc: 0.8150\n",
      "Epoch 93/100\n",
      "3340/3340 [==============================] - 1s 333us/step - loss: 0.5532 - acc: 0.8102\n",
      "Epoch 94/100\n",
      "3340/3340 [==============================] - 1s 332us/step - loss: 0.5359 - acc: 0.8168\n",
      "Epoch 95/100\n",
      "3340/3340 [==============================] - 1s 334us/step - loss: 0.5752 - acc: 0.8054\n",
      "Epoch 96/100\n",
      "3340/3340 [==============================] - 1s 332us/step - loss: 0.5438 - acc: 0.8162\n",
      "Epoch 97/100\n",
      "3340/3340 [==============================] - 1s 335us/step - loss: 0.5592 - acc: 0.8135\n",
      "Epoch 98/100\n",
      "3340/3340 [==============================] - 1s 337us/step - loss: 0.5430 - acc: 0.8195\n",
      "Epoch 99/100\n",
      "3340/3340 [==============================] - 1s 337us/step - loss: 0.5439 - acc: 0.8126\n",
      "Epoch 100/100\n",
      "3340/3340 [==============================] - 1s 350us/step - loss: 0.5312 - acc: 0.8213\n",
      "836/836 [==============================] - 0s 241us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2417039654471658, 0.6794258373205742]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(np.reshape(x3_train, (-1,20,20,1)), c_train, epochs = 100, batch_size=32)\n",
    "model5.evaluate(np.reshape(x3_test, (-1,20,20,1)), c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I got an accuracy of about 63-67% training and testing my best network on three different fonts. That's about 2% worse than in the 1 font case.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing how a model performs when used on a different font from the one it was trained on.\n",
    "nf4 = makeDataFrame(path, \"HARLOW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4_s, Y4_s = dfTransform(nf4)\n",
    "char4_to_ix4, ix4_to_char4 = CharToIxArr(Y4_s)\n",
    "d = makeOneHot(char4_to_ix4, ix4_to_char4)\n",
    "x4_train, x4_test, d_train, d_test = train_test_split(X4_s, d, train_size = 0.8, test_size = 0.2)\n",
    "len(char4_to_ix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Since VIVALDI and HARLOW have the same number of characters I'm going to build and train a model on HARLOW then test it on VIVALDI to see how it performs. (when I made char_to_ix arrays they both had a length of 238)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "761/761 [==============================] - 1s 926us/step - loss: 5.4898 - acc: 0.0026\n",
      "Epoch 2/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 5.4604 - acc: 0.0053\n",
      "Epoch 3/100\n",
      "761/761 [==============================] - 0s 342us/step - loss: 5.4114 - acc: 0.0184\n",
      "Epoch 4/100\n",
      "761/761 [==============================] - 0s 339us/step - loss: 5.2689 - acc: 0.0394\n",
      "Epoch 5/100\n",
      "761/761 [==============================] - 0s 337us/step - loss: 4.8995 - acc: 0.0644\n",
      "Epoch 6/100\n",
      "761/761 [==============================] - 0s 332us/step - loss: 4.3241 - acc: 0.1380\n",
      "Epoch 7/100\n",
      "761/761 [==============================] - 0s 335us/step - loss: 3.6497 - acc: 0.2260\n",
      "Epoch 8/100\n",
      "761/761 [==============================] - 0s 350us/step - loss: 2.9634 - acc: 0.3154\n",
      "Epoch 9/100\n",
      "761/761 [==============================] - 0s 341us/step - loss: 2.3064 - acc: 0.4455\n",
      "Epoch 10/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 1.9504 - acc: 0.4888\n",
      "Epoch 11/100\n",
      "761/761 [==============================] - 0s 341us/step - loss: 1.6422 - acc: 0.5742\n",
      "Epoch 12/100\n",
      "761/761 [==============================] - 0s 347us/step - loss: 1.4560 - acc: 0.6084\n",
      "Epoch 13/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 1.3200 - acc: 0.6465\n",
      "Epoch 14/100\n",
      "761/761 [==============================] - 0s 346us/step - loss: 1.1987 - acc: 0.6610\n",
      "Epoch 15/100\n",
      "761/761 [==============================] - 0s 345us/step - loss: 1.0156 - acc: 0.6938\n",
      "Epoch 16/100\n",
      "761/761 [==============================] - 0s 350us/step - loss: 0.9937 - acc: 0.7385\n",
      "Epoch 17/100\n",
      "761/761 [==============================] - 0s 346us/step - loss: 0.8851 - acc: 0.7477\n",
      "Epoch 18/100\n",
      "761/761 [==============================] - 0s 348us/step - loss: 0.8129 - acc: 0.7608\n",
      "Epoch 19/100\n",
      "761/761 [==============================] - 0s 346us/step - loss: 0.7856 - acc: 0.7608\n",
      "Epoch 20/100\n",
      "761/761 [==============================] - 0s 350us/step - loss: 0.7710 - acc: 0.7727\n",
      "Epoch 21/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 0.6927 - acc: 0.7976\n",
      "Epoch 22/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.6527 - acc: 0.8042\n",
      "Epoch 23/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.6494 - acc: 0.7924\n",
      "Epoch 24/100\n",
      "761/761 [==============================] - 0s 336us/step - loss: 0.6339 - acc: 0.8160\n",
      "Epoch 25/100\n",
      "761/761 [==============================] - 0s 333us/step - loss: 0.5998 - acc: 0.8134\n",
      "Epoch 26/100\n",
      "761/761 [==============================] - 0s 341us/step - loss: 0.5705 - acc: 0.8265\n",
      "Epoch 27/100\n",
      "761/761 [==============================] - 0s 335us/step - loss: 0.5780 - acc: 0.8173\n",
      "Epoch 28/100\n",
      "761/761 [==============================] - 0s 340us/step - loss: 0.5214 - acc: 0.8213\n",
      "Epoch 29/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.5422 - acc: 0.8371\n",
      "Epoch 30/100\n",
      "761/761 [==============================] - 0s 354us/step - loss: 0.5288 - acc: 0.8305\n",
      "Epoch 31/100\n",
      "761/761 [==============================] - 0s 357us/step - loss: 0.5100 - acc: 0.8371\n",
      "Epoch 32/100\n",
      "761/761 [==============================] - 0s 347us/step - loss: 0.4838 - acc: 0.8449\n",
      "Epoch 33/100\n",
      "761/761 [==============================] - 0s 337us/step - loss: 0.4265 - acc: 0.8647\n",
      "Epoch 34/100\n",
      "761/761 [==============================] - 0s 333us/step - loss: 0.4641 - acc: 0.8449\n",
      "Epoch 35/100\n",
      "761/761 [==============================] - 0s 338us/step - loss: 0.4944 - acc: 0.8568\n",
      "Epoch 36/100\n",
      "761/761 [==============================] - 0s 335us/step - loss: 0.4320 - acc: 0.8528\n",
      "Epoch 37/100\n",
      "761/761 [==============================] - 0s 357us/step - loss: 0.4157 - acc: 0.8647\n",
      "Epoch 38/100\n",
      "761/761 [==============================] - 0s 336us/step - loss: 0.4228 - acc: 0.8555\n",
      "Epoch 39/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.4017 - acc: 0.8647\n",
      "Epoch 40/100\n",
      "761/761 [==============================] - 0s 345us/step - loss: 0.4228 - acc: 0.8607\n",
      "Epoch 41/100\n",
      "761/761 [==============================] - 0s 389us/step - loss: 0.3868 - acc: 0.8515\n",
      "Epoch 42/100\n",
      "761/761 [==============================] - 0s 470us/step - loss: 0.3932 - acc: 0.8594\n",
      "Epoch 43/100\n",
      "761/761 [==============================] - 0s 447us/step - loss: 0.3368 - acc: 0.8778\n",
      "Epoch 44/100\n",
      "761/761 [==============================] - 0s 477us/step - loss: 0.3511 - acc: 0.8739\n",
      "Epoch 45/100\n",
      "761/761 [==============================] - 0s 489us/step - loss: 0.3890 - acc: 0.8686\n",
      "Epoch 46/100\n",
      "761/761 [==============================] - 0s 434us/step - loss: 0.3372 - acc: 0.8844\n",
      "Epoch 47/100\n",
      "761/761 [==============================] - 0s 393us/step - loss: 0.3424 - acc: 0.8896\n",
      "Epoch 48/100\n",
      "761/761 [==============================] - 0s 382us/step - loss: 0.3681 - acc: 0.8752\n",
      "Epoch 49/100\n",
      "761/761 [==============================] - 0s 399us/step - loss: 0.3252 - acc: 0.8922\n",
      "Epoch 50/100\n",
      "761/761 [==============================] - 0s 396us/step - loss: 0.3068 - acc: 0.8962\n",
      "Epoch 51/100\n",
      "761/761 [==============================] - 0s 380us/step - loss: 0.3383 - acc: 0.8817\n",
      "Epoch 52/100\n",
      "761/761 [==============================] - 0s 439us/step - loss: 0.3074 - acc: 0.8936\n",
      "Epoch 53/100\n",
      "761/761 [==============================] - 0s 466us/step - loss: 0.2719 - acc: 0.9028\n",
      "Epoch 54/100\n",
      "761/761 [==============================] - 0s 372us/step - loss: 0.3008 - acc: 0.8936\n",
      "Epoch 55/100\n",
      "761/761 [==============================] - 0s 373us/step - loss: 0.2570 - acc: 0.9093\n",
      "Epoch 56/100\n",
      "761/761 [==============================] - 0s 353us/step - loss: 0.2939 - acc: 0.9028\n",
      "Epoch 57/100\n",
      "761/761 [==============================] - 0s 373us/step - loss: 0.2822 - acc: 0.9080\n",
      "Epoch 58/100\n",
      "761/761 [==============================] - 0s 356us/step - loss: 0.2765 - acc: 0.8962\n",
      "Epoch 59/100\n",
      "761/761 [==============================] - 0s 351us/step - loss: 0.2687 - acc: 0.9041\n",
      "Epoch 60/100\n",
      "761/761 [==============================] - 0s 336us/step - loss: 0.2825 - acc: 0.9028\n",
      "Epoch 61/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.2426 - acc: 0.9067\n",
      "Epoch 62/100\n",
      "761/761 [==============================] - 0s 352us/step - loss: 0.2555 - acc: 0.9120\n",
      "Epoch 63/100\n",
      "761/761 [==============================] - 0s 356us/step - loss: 0.2649 - acc: 0.9028\n",
      "Epoch 64/100\n",
      "761/761 [==============================] - 0s 365us/step - loss: 0.2589 - acc: 0.9054\n",
      "Epoch 65/100\n",
      "761/761 [==============================] - 0s 343us/step - loss: 0.2328 - acc: 0.9159\n",
      "Epoch 66/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 0.2321 - acc: 0.9185\n",
      "Epoch 67/100\n",
      "761/761 [==============================] - 0s 348us/step - loss: 0.2255 - acc: 0.9238\n",
      "Epoch 68/100\n",
      "761/761 [==============================] - 0s 343us/step - loss: 0.2183 - acc: 0.9159\n",
      "Epoch 69/100\n",
      "761/761 [==============================] - 0s 343us/step - loss: 0.2465 - acc: 0.9251\n",
      "Epoch 70/100\n",
      "761/761 [==============================] - 0s 353us/step - loss: 0.2476 - acc: 0.9041\n",
      "Epoch 71/100\n",
      "761/761 [==============================] - 0s 358us/step - loss: 0.2064 - acc: 0.9238\n",
      "Epoch 72/100\n",
      "761/761 [==============================] - 0s 344us/step - loss: 0.2205 - acc: 0.9198\n",
      "Epoch 73/100\n",
      "761/761 [==============================] - 0s 353us/step - loss: 0.2335 - acc: 0.9120\n",
      "Epoch 74/100\n",
      "761/761 [==============================] - 0s 365us/step - loss: 0.2109 - acc: 0.9264\n",
      "Epoch 75/100\n",
      "761/761 [==============================] - 0s 343us/step - loss: 0.2102 - acc: 0.9238\n",
      "Epoch 76/100\n",
      "761/761 [==============================] - 0s 337us/step - loss: 0.1882 - acc: 0.9330\n",
      "Epoch 77/100\n",
      "761/761 [==============================] - 0s 334us/step - loss: 0.2217 - acc: 0.9198\n",
      "Epoch 78/100\n",
      "761/761 [==============================] - 0s 335us/step - loss: 0.2041 - acc: 0.9146\n",
      "Epoch 79/100\n",
      "761/761 [==============================] - 0s 345us/step - loss: 0.1949 - acc: 0.9330\n",
      "Epoch 80/100\n",
      "761/761 [==============================] - 0s 346us/step - loss: 0.1944 - acc: 0.9212\n",
      "Epoch 81/100\n",
      "761/761 [==============================] - 0s 341us/step - loss: 0.1750 - acc: 0.9330\n",
      "Epoch 82/100\n",
      "761/761 [==============================] - 0s 364us/step - loss: 0.2090 - acc: 0.9212\n",
      "Epoch 83/100\n",
      "761/761 [==============================] - 0s 502us/step - loss: 0.1770 - acc: 0.9356\n",
      "Epoch 84/100\n",
      "761/761 [==============================] - 0s 476us/step - loss: 0.1896 - acc: 0.9238\n",
      "Epoch 85/100\n",
      "761/761 [==============================] - 0s 463us/step - loss: 0.1926 - acc: 0.9212\n",
      "Epoch 86/100\n",
      "761/761 [==============================] - 0s 343us/step - loss: 0.1673 - acc: 0.9382\n",
      "Epoch 87/100\n",
      "761/761 [==============================] - 0s 350us/step - loss: 0.1941 - acc: 0.9409\n",
      "Epoch 88/100\n",
      "761/761 [==============================] - 0s 345us/step - loss: 0.1604 - acc: 0.9369\n",
      "Epoch 89/100\n",
      "761/761 [==============================] - 0s 359us/step - loss: 0.2026 - acc: 0.9264\n",
      "Epoch 90/100\n",
      "761/761 [==============================] - 0s 349us/step - loss: 0.1804 - acc: 0.9317\n",
      "Epoch 91/100\n",
      "761/761 [==============================] - 0s 360us/step - loss: 0.1562 - acc: 0.9422\n",
      "Epoch 92/100\n",
      "761/761 [==============================] - 0s 427us/step - loss: 0.1803 - acc: 0.9290\n",
      "Epoch 93/100\n",
      "761/761 [==============================] - 0s 472us/step - loss: 0.1584 - acc: 0.9435\n",
      "Epoch 94/100\n",
      "761/761 [==============================] - 0s 444us/step - loss: 0.1809 - acc: 0.9304\n",
      "Epoch 95/100\n",
      "761/761 [==============================] - 0s 379us/step - loss: 0.1537 - acc: 0.9369\n",
      "Epoch 96/100\n",
      "761/761 [==============================] - 0s 377us/step - loss: 0.1750 - acc: 0.9382\n",
      "Epoch 97/100\n",
      "761/761 [==============================] - 0s 467us/step - loss: 0.1662 - acc: 0.9369\n",
      "Epoch 98/100\n",
      "761/761 [==============================] - 0s 444us/step - loss: 0.1544 - acc: 0.9396\n",
      "Epoch 99/100\n",
      "761/761 [==============================] - 0s 421us/step - loss: 0.1542 - acc: 0.9422\n",
      "Epoch 100/100\n",
      "761/761 [==============================] - 0s 363us/step - loss: 0.1867 - acc: 0.9356\n",
      "191/191 [==============================] - 0s 815us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42757419500675503, 0.9214659688984537]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = makeImprovedModel(char4_to_ix4)\n",
    "model6.fit(np.reshape(x4_train, (-1,20,20,1)), d_train, epochs = 100, batch_size=32)\n",
    "model6.evaluate(np.reshape(x4_test, (-1,20,20,1)), d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I get an excellent fit with the HARLOW data. 88-95% accurate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf5 = makeDataFrame(path, \"VIVALDI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5_s, Y5_s = dfTransform(nf5)\n",
    "char5_to_ix5, ix5_to_char5 = CharToIxArr(Y5_s)\n",
    "e = makeOneHot(char5_to_ix5, ix5_to_char5)\n",
    "x5_train, x5_test, e_train, e_test = train_test_split(X5_s, e, train_size = 0.8, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 0s 130us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.853086166981003, 0.10471204141671744]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate(np.reshape(x5_test, (-1,20,20,1)), e_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The model gets 10-16% accuracy when attempting to predict the VIVALDI characters when trained on the HARLOW characters.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 249.14 775.00\" width=\"249pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 245.1416,-771 245.1416,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4954694040 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4954694040</title>\n",
       "<polygon fill=\"none\" points=\"54.3013,-657.5 54.3013,-693.5 186.8403,-693.5 186.8403,-657.5 54.3013,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-671.3\">conv2d_11: Conv2D</text>\n",
       "</g>\n",
       "<!-- 4954696336 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4954696336</title>\n",
       "<polygon fill=\"none\" points=\"11.522,-584.5 11.522,-620.5 229.6196,-620.5 229.6196,-584.5 11.522,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-598.3\">max_pooling2d_11: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 4954694040&#45;&gt;4954696336 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4954694040-&gt;4954696336</title>\n",
       "<path d=\"M120.5708,-657.4551C120.5708,-649.3828 120.5708,-639.6764 120.5708,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-630.5903 120.5708,-620.5904 117.0709,-630.5904 124.0709,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5039358248 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5039358248</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 241.1416,-547.5 241.1416,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-525.3\">spatial_dropout2d_7: SpatialDropout2D</text>\n",
       "</g>\n",
       "<!-- 4954696336&#45;&gt;5039358248 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4954696336-&gt;5039358248</title>\n",
       "<path d=\"M120.5708,-584.4551C120.5708,-576.3828 120.5708,-566.6764 120.5708,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-557.5903 120.5708,-547.5904 117.0709,-557.5904 124.0709,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5039359928 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5039359928</title>\n",
       "<polygon fill=\"none\" points=\"54.0449,-438.5 54.0449,-474.5 187.0967,-474.5 187.0967,-438.5 54.0449,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-452.3\">conv2d_12: Conv2D</text>\n",
       "</g>\n",
       "<!-- 5039358248&#45;&gt;5039359928 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5039358248-&gt;5039359928</title>\n",
       "<path d=\"M120.5708,-511.4551C120.5708,-503.3828 120.5708,-493.6764 120.5708,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-484.5903 120.5708,-474.5904 117.0709,-484.5904 124.0709,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954798512 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4954798512</title>\n",
       "<polygon fill=\"none\" points=\"11.2656,-365.5 11.2656,-401.5 229.876,-401.5 229.876,-365.5 11.2656,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-379.3\">max_pooling2d_12: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 5039359928&#45;&gt;4954798512 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5039359928-&gt;4954798512</title>\n",
       "<path d=\"M120.5708,-438.4551C120.5708,-430.3828 120.5708,-420.6764 120.5708,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-411.5903 120.5708,-401.5904 117.0709,-411.5904 124.0709,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954873136 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4954873136</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 241.1416,-328.5 241.1416,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-306.3\">spatial_dropout2d_8: SpatialDropout2D</text>\n",
       "</g>\n",
       "<!-- 4954798512&#45;&gt;4954873136 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>4954798512-&gt;4954873136</title>\n",
       "<path d=\"M120.5708,-365.4551C120.5708,-357.3828 120.5708,-347.6764 120.5708,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-338.5903 120.5708,-328.5904 117.0709,-338.5904 124.0709,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954893672 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>4954893672</title>\n",
       "<polygon fill=\"none\" points=\"64.938,-219.5 64.938,-255.5 176.2036,-255.5 176.2036,-219.5 64.938,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-233.3\">flatten_6: Flatten</text>\n",
       "</g>\n",
       "<!-- 4954873136&#45;&gt;4954893672 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>4954873136-&gt;4954893672</title>\n",
       "<path d=\"M120.5708,-292.4551C120.5708,-284.3828 120.5708,-274.6764 120.5708,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-265.5903 120.5708,-255.5904 117.0709,-265.5904 124.0709,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954892944 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>4954892944</title>\n",
       "<polygon fill=\"none\" points=\"56.769,-146.5 56.769,-182.5 184.3726,-182.5 184.3726,-146.5 56.769,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-160.3\">dropout_6: Dropout</text>\n",
       "</g>\n",
       "<!-- 4954893672&#45;&gt;4954892944 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>4954893672-&gt;4954892944</title>\n",
       "<path d=\"M120.5708,-219.4551C120.5708,-211.3828 120.5708,-201.6764 120.5708,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-192.5903 120.5708,-182.5904 117.0709,-192.5904 124.0709,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4918533592 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>4918533592</title>\n",
       "<polygon fill=\"none\" points=\"65.2012,-73.5 65.2012,-109.5 175.9404,-109.5 175.9404,-73.5 65.2012,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-87.3\">dense_11: Dense</text>\n",
       "</g>\n",
       "<!-- 4954892944&#45;&gt;4918533592 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>4954892944-&gt;4918533592</title>\n",
       "<path d=\"M120.5708,-146.4551C120.5708,-138.3828 120.5708,-128.6764 120.5708,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-119.5903 120.5708,-109.5904 117.0709,-119.5904 124.0709,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954424600 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>4954424600</title>\n",
       "<polygon fill=\"none\" points=\"64.9448,-.5 64.9448,-36.5 176.1968,-36.5 176.1968,-.5 64.9448,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-14.3\">dense_12: Dense</text>\n",
       "</g>\n",
       "<!-- 4918533592&#45;&gt;4954424600 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>4918533592-&gt;4954424600</title>\n",
       "<path d=\"M120.5708,-73.4551C120.5708,-65.3828 120.5708,-55.6764 120.5708,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-46.5903 120.5708,-36.5904 117.0709,-46.5904 124.0709,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4954695608 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>4954695608</title>\n",
       "<polygon fill=\"none\" points=\"77.5708,-730.5 77.5708,-766.5 163.5708,-766.5 163.5708,-730.5 77.5708,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5708\" y=\"-744.3\">4954695608</text>\n",
       "</g>\n",
       "<!-- 4954695608&#45;&gt;4954694040 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4954695608-&gt;4954694040</title>\n",
       "<path d=\"M120.5708,-730.4551C120.5708,-722.3828 120.5708,-712.6764 120.5708,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"124.0709,-703.5903 120.5708,-693.5904 117.0709,-703.5904 124.0709,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model6).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***That was not what I wanted.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "        0., 14., 15.,  0., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n",
       "       52., 53., 54.,  0., 56., 57., 58., 59., 60., 61., 62., 63., 64.,\n",
       "       65., 66., 67., 68., 69., 70., 71., 72., 73.,  0., 75., 76.,  0.,\n",
       "       78., 79., 80., 81.,  0., 83., 84., 85., 86., 87., 88.,  0., 90.,\n",
       "       91., 92., 93.,  0., 95., 96., 97., 98., 99.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Junchen helped me extensively with this as well.\n",
    "# Display images of misclassified characters.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# First collect the instances and their unicode codes\n",
    "y_pred = model6.predict(np.reshape(x5_test, (-1,20,20,1)), batch_size=32, verbose = 0, steps = None)\n",
    "misses = np.zeros(100)\n",
    "count=0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i] != e_test[i]).all():\n",
    "        misses[count] = i\n",
    "    count+=1\n",
    "    if(count == 100):\n",
    "        break\n",
    "        \n",
    "plt.figure(figsize = (10,10))\n",
    "misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/westleykirkham/PycharmProjects/Translator/venv/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 92 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the characters\n",
    "for i in misses:\n",
    "    ii = int(i)\n",
    "    #print(ii)\n",
    "    plt.subplot(10,10,ii+1)\n",
    "    plt.imshow(np.reshape(X5_s[ii], (20,20)), cmap = 'Greys', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***f and capital A are very similar. In many instances, it's the letters or shapes that are close in appearance and form, but have small dots or dashes attached to them that the network misclassifies. Various forms of u with dots and omlauts, quotations, blocky shapes, es, as, os, etc. are all things the network must be unsure of, as they're misclassified together. So, it's essentially guessing between a handful of options when these letters come up because they're too close for it to be able to tell the difference.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
